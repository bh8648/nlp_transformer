{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e2df62",
   "metadata": {},
   "source": [
    "#### Note: 이 장에서는 분산 인프라에서 대규모 언어 모델을 훈련하기 위한 대용량 데이터셋과 스크립트를 만듭니다. 따라서 이 노트북의 모든 단계가 코랩이나 캐글 같은 플랫폼에서 실행 가능한 것은 아닙니다. 중요 지점에서 단계를 축소하거나 분산 훈련 스크립트를 만들 때 참고용으로 이 노트북을 사용하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b129f07",
   "metadata": {},
   "source": [
    "#### Note: 이 책의 다른 코드와 달리 이 장의 훈련 코드는 다중 GPU에서 스크립트로 실행하도록 만들어졌습니다. CodeParrot을 직접 훈련하려면 트랜스포머스 저장소(https://oreil.ly/ZyPPR)에 있는 스크립트를 사용하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41308df",
   "metadata": {},
   "source": [
    "코파일럿, TabNine, Kite 등 트랜스포머를 사용해 코드 자동 완성을 수행하는 애플리케이션들이 있다.  \n",
    "  \n",
    "5장에서 GPT 모델을 사용해 고품질 텍스트를 생성하는 방법을 알아보았으므로  \n",
    "이 장에서는 이 둘을 연결해 파이썬 코드를 생성하는 GPT 유사 모델을 직접 만들어보자.(이 모델을 CodeParrot이라 칭하겠다)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67790aa1",
   "metadata": {},
   "source": [
    "원하는 데이터가 모두 있을 때 무엇을 할 수 있는지 알아보기.  \n",
    "사전 훈련 단계 자체를 살펴보고, 트랜스포머 모델을 밑바닥부터 훈련하는 법을 배운다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5778be",
   "metadata": {},
   "source": [
    "1. 대용량 데이터셋 수집과 처리  \n",
    "2. 자신만의 데이터셋을 위한 사용자 정의 토크나이저 만들기  \n",
    "3. 여러 GPU에서 대규모로 모델 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba2d64",
   "metadata": {},
   "source": [
    "파라미터가 수십억 개인 대규모 모델을 효과적으로 훈련하려면 분산 훈련을 위한 특별한 도구가 필요하다.   \n",
    "  \n",
    "분산 훈련은 트랜스포머스의 Trainer도 지원하지만, 지금이야말로 액셀러레이트 Accelerate라는 강력한 파이토치 라이브러리를 소개할 때다.  \n",
    "이 장의 훈련 코드는 다중 GPU에서 스크립트로 실행해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799de4d",
   "metadata": {},
   "source": [
    "# 10.1 대규모 데이터셋 수집하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42b6fe",
   "metadata": {},
   "source": [
    "## 10.1.1 대규모 말뭉치 구축의 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24b799",
   "metadata": {},
   "source": [
    "사전 훈련된 모델을 사용하면 해당 모델의 토크나이저를 사용해야 한다.  \n",
    "하지만 다른 도메인의 말뭉치에서 훈련한 토크나이저를 사용하는 것은 대개 최선의 결정이 아니다.  \n",
    "  \n",
    "가령 법률문서에서 훈련한 GPT의 사전 훈련 토크나이저를 다른 언어, 음악 악보나 DNA 시퀀스와 같이 완전히 다른 시퀀스에 사용하면 토큰화 결과가 좋지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d447413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
    "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3613ddeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 크기: 116.5M parameters\n",
      "GPT2 크기: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT 크기: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
    "print(f\"GPT2 크기: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f83ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2844ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_pipeline_outputs(pipe, prompt, num_return_sequences):\n",
    "    out = pipe(prompt, num_return_sequences=num_return_sequences,\n",
    "               clean_up_tokenization_spaces=True)\n",
    "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ae78896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h e l l o\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1h 1e 1l 1l 1o'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zzz = ' '\n",
    "temp = [\"h\", \"e\", \"l\", \"l\", \"o\"]\n",
    "print(zzz.join(temp))\n",
    "zzz.join(\"1\" + s for s in temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6dbad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jj/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 자동 완성:\n",
      "1.\n",
      "When they came back, she 'd want to know what he was going to do. \n",
      " \" it 'll teach your brother a lesson, \" he said, and then he looked pointedly at my arm and the blood he 'd poured over my skin. \"\n",
      "2.\n",
      "When they came back, a little boy with a black nose and brown eyes who had started hanging around the neighborhood on special occasions and who lived off of the property. he wore a football jersey with the letters \" the black diamond \" written on it.\n",
      "3.\n",
      "When they came back, and they were still there. \" \n",
      " jake nodded. \n",
      " i shrugged and sat back down. \" so then, they did come back, \" i said, thinking. \" but... \" \n",
      " \" what's her name?\n",
      "\n",
      "GPT2 자동 완성:\n",
      "1.\n",
      "When they came back the body was still on fire, and there were a lot of people who had died. The smell of diesel had been getting worse. No one was doing well. The streets were deserted, with no water and no running water\n",
      "2.\n",
      "When they came back to the door, they came back with the fire extinguisher.\n",
      "\n",
      "The man, who was carrying only a small gun, shot at the gas and the man's car. As soon as the gas came out of the\n",
      "3.\n",
      "When they came back to the ship, Raine had a choice of waiting or risking an extra ship while he and his crew kept him on the island. Raine got the choice of waiting or having the opportunity to go see the island where he\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\\nWhen they came back\"\n",
    "print(\"GPT 자동 완성:\\n\" + enum_pipeline_outputs(generation_gpt, prompt, 3))\n",
    "print(\"\")\n",
    "print(\"GPT2 자동 완성:\\n\" + enum_pipeline_outputs(generation_gpt2, prompt, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47d5b2",
   "metadata": {},
   "source": [
    "## 10.1.2 사용자 정의 코드 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734bf6e6",
   "metadata": {},
   "source": [
    "파일을 추출하는 과정은 다음과 같이 TransCoder 구현을 참고함  \n",
    "https://oreil.ly/vih2m  \n",
    "1. 구글 클라우드 계정을 만듦(무료 크레딧으로 충분)  \n",
    "2. 계정 아래 구글 빅쿼리 프로젝트를 만듦.  \n",
    "3. 이 프로젝트에서 데이터셋을 만듦.  \n",
    "4. 이 프로젝트에서 SQL 요청 결과를 저장할 테이블을 만듦.  \n",
    "5. github_repos에서 다음 SQL 쿼리를 준비하고 실행한다.(쿼리 결과를 저장하려면 [더보기] > [쿼리 설정]을 선택하고 '쿼리 결과의 대상 테이블 설정'을 체크하고 테이블 이름을 지정한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321c380",
   "metadata": {},
   "source": [
    "## 10.1.3 대용량 데이터셋 다루기\n",
    "작은 컴퓨터에서 대용량 데이터셋을 다룰 때 발생하는 제약 사항을 해결하는 데 데이터셋이 어떤 도움이 되는지 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4a339",
   "metadata": {},
   "source": [
    "컴퓨터의 RAM보다 큰 대용량 데이터셋을 로딩하는 작업은 어려움.  \n",
    "표준적인 랩톱이나 데스크톱 컴퓨터의 메모리로 로드하기 어렵다.  \n",
    "  \n",
    "데이터셋은 이런 문제를 고려해 설계됨.  \n",
    "#### 메모리와 하드 드라이브 공간의 제약을 해결할 수 있도록 메모리 매핑과 스트리밍(streaming)기능을 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccdd0b",
   "metadata": {},
   "source": [
    "## 메모리 매핑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b67427f",
   "metadata": {},
   "source": [
    "메모리 제약을 극복하기 위해 데이터셋은 zero-copy와 zero-overhead 메모리 매핑을 위한 메커니즘을 사용하며 기본적으로 활성화된다.  \n",
    "각 데이터셋은 메모리 내용이 바로 반영되는 하나의 파일로 디스크에 캐싱된다.  \n",
    "데이터셋을 메모리로 로딩하지 않고, 데이터셋은 이 파일에서 읽기 전용 포인터를 열어 메모리 대신 이를 사용한다.  \n",
    "#### 근본적으로 하드드라이브를 메모리의 확장으로 사용하는 셈이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e11098",
   "metadata": {},
   "source": [
    "이때까지는 허깅페이스 허브에 있는 원격 데이터셋을 가져올 때 대부분 데이터셋을 사용했지만 여기는 로컬 codeparrot 저장소에 저장된, 50GB의 압축된 JSON 파일을 직접 로드한다.  \n",
    "먼저 JSON 파일의 압축을 풀어야 한다. -> 하지만 데이터셋이 이를 알아서 처리한다.  \n",
    "#### 주의할 점은 180GB의 디스크 공간이 필요하다. 다만 RAM은 거의 사용하지 않는다.  \n",
    "데이터셋의 다운로드 설정에 delete_extracted=True를 지정하면 더 이상 필요하지 않은 모든 파일을 즉시 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f6d503",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DownloadConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03d02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_config = DownloadConfig(delete_extracted=True)\n",
    "# dataset = load_dataset(\"/mnt/hdd_repo1/codeparrot\", split=\"train\",\n",
    "#                        download_config=download_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6bc22",
   "metadata": {},
   "source": [
    "데이터셋은 내부적으로 압축된 JSON 파일을 최적화된 캐시 파일 하나에 로드해서 내용을 모두 추출하고 읽어들인다.  \n",
    "이 데이터셋이 로드되면 용량이 얼마나 되는지 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be140df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil # 메모리 추적하는 라이브러리\n",
    "\n",
    "# print(f\"데이터셋에 있는 파이썬 파일의 개수 : {len(dataset)}\")\n",
    "# ds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n",
    "# # os.stat.st_size는 바이트 단위이므로 GB로 바꿉니다.\n",
    "# print(f\"데이터셋 크기 (캐시 파일) : {ds_size / 2**30:.2f} GB\")\n",
    "# # process.memory_info는 바이트 단위이므로 MB로 바꿉니다.\n",
    "# print(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a30c397",
   "metadata": {},
   "source": [
    "데이터셋은 일반적인 램 메모리 용량보다 훨씬 더 크다.  \n",
    "하지만 여전히 로드할 수 있고 실제로 사용하는 메모리가 매우 적다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2644ea",
   "metadata": {},
   "source": [
    "#### 하지만 전체 데이터셋을 로컬에 저장할 만큼 여유 공간이 충분하지 않다면 어떻게 될까?\n",
    "다행히 전체 데이터셋을 로컬에 저장하지 않는 방법이 있는데, 바로 데이터셋의 스트리밍이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb1c00",
   "metadata": {},
   "source": [
    "## 스트리밍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f68827",
   "metadata": {},
   "source": [
    "(1TB 또는 그 이상 되는) 일부 데이터셋은 표준적인 하드 드라이브가 있더라도 다루기 어렵다.  \n",
    "이 경우 사용하는 서버의 용량을 높이는 대신 데이터셋을 스트리밍할 수 있다.  \n",
    "JSONL (JSON Lines), CSV, (원시 포맷 또는 zip, gzip, zstandard 압축된) 텍스트 같이 한 줄씩 읽는 여러 종류의 압축 또는 비압축 파일 포맷을 데이터셋으로 처리할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b4fa4",
   "metadata": {},
   "source": [
    "캐시 파일을 만들지 않고 압축된 JSON 파일을 바로 데이터셋으로 로드해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f80a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamed_dataset = load_dataset(\"/mnt/hdd_repo1/codeparrot\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df5744",
   "metadata": {},
   "source": [
    "스트리밍 모드는 압축된 JSON 파일을 열어 동적으로 읽는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842e87a",
   "metadata": {},
   "source": [
    "데이터셋은 이제 IterableDataset 객체이므로 streamed_dataset[1234] 처럼 랜덤하게 원소에 접근할 수는 없지만 대신 next(iter(streamed_dataset)) 같은 방식으로 순서대로 읽어야 한다.  \n",
    "  \n",
    "  \n",
    "shuffle() 같은 메서드는 여전히 사용 가능하지만, 샘플 버퍼를 추출하고 이 버퍼 안에서 랜덤하게 섞는 식으로 동작한다.(버퍼 크기는 조정 가능하다.)  \n",
    "원시 파일이 여러 개(이 경우. 184개)일 때, shuffle()은 반복마다 파일 순서를 랜덤하게 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd370f51",
   "metadata": {},
   "source": [
    "스트리밍 데이터셋의 샘플은 스트리밍하지 않는 데이터셋의 샘플과 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69432610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(streamed_dataset)\n",
    "\n",
    "# print(dataset[0] == next(iterator))\n",
    "# print(dataset[1] == next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a160d0e",
   "metadata": {},
   "source": [
    "#### 스트리밍 데이터셋의 장점: 데이터셋을 로드할 때 하드 드라이브에 캐시 파일이 생성되지 않고 (많은 양의) 메모리가 필요하지 않다는 것.  \n",
    "새로운 샘플 배치가 필요할 때 원시 파일을 즉시 추출하고 읽어 들여 해당 배치만 메모리에 로드한다.  \n",
    "-> 그래서 데이터셋의 메모리 사용량이 크게 줄어든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601055c4",
   "metadata": {},
   "source": [
    "#### 한 단계 더 나아가 로컬 데이터셋이 아니라 허브에 있는 데이터셋을 지정할 수 있다. 그러면 원시 파일을 로컬에 다운로드하지 않고 샘플을 직접 다운로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dae7927",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6ff274ead84c7bbf392cc60f707dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "remote_dataset = load_dataset(\"transformersbook/codeparrot\", split=\"train\",\n",
    "                              streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f596b15",
   "metadata": {},
   "source": [
    "이 데이터셋은 이전과 완전히 똑같이 동작한다. 하지만 내부적으로 샘플을 동적으로 다운로드한다.  \n",
    "이렇게 설정하면 (대부분의) 작은 서버에서도 아주 큰 대용량 데이터셋을 사용할 수 있다.  \n",
    "데이터셋을 훈련 분할과 검증 분할로 나눠 허깅페이스 허브에 업로드하고 스트리밍으로 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd0a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(streamed_dataset)\n",
    "# print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2800a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'repo_name': 'ahmedbodi/AutobahnPython', 'path': 'examples/asyncio/websocket/echo/client_coroutines.py', 'copies': '13', 'size': '2044', 'content': '###############################################################################\\n##\\n##  Copyright (C) 2013-2014 Tavendo GmbH\\n##\\n##  Licensed under the Apache License, Version 2.0 (the \"License\");\\n##  you may not use this file except in compliance with the License.\\n##  You may obtain a copy of the License at\\n##\\n##      http://www.apache.org/licenses/LICENSE-2.0\\n##\\n##  Unless required by applicable law or agreed to in writing, software\\n##  distributed under the License is distributed on an \"AS IS\" BASIS,\\n##  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n##  See the License for the specific language governing permissions and\\n##  limitations under the License.\\n##\\n###############################################################################\\n\\nfrom autobahn.asyncio.websocket import WebSocketClientProtocol, \\\\\\n                                       WebSocketClientFactory\\n\\nimport asyncio\\n\\n\\n\\nclass MyClientProtocol(WebSocketClientProtocol):\\n\\n   def onConnect(self, response):\\n      print(\"Server connected: {0}\".format(response.peer))\\n\\n   @asyncio.coroutine\\n   def onOpen(self):\\n      print(\"WebSocket connection open.\")\\n\\n      ## start sending messages every second ..\\n      while True:\\n         self.sendMessage(u\"Hello, world!\".encode(\\'utf8\\'))\\n         self.sendMessage(b\"\\\\x00\\\\x01\\\\x03\\\\x04\", isBinary = True)\\n         yield from asyncio.sleep(1)\\n\\n   def onMessage(self, payload, isBinary):\\n      if isBinary:\\n         print(\"Binary message received: {0} bytes\".format(len(payload)))\\n      else:\\n         print(\"Text message received: {0}\".format(payload.decode(\\'utf8\\')))\\n\\n   def onClose(self, wasClean, code, reason):\\n      print(\"WebSocket connection closed: {0}\".format(reason))\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n\\n   import asyncio\\n\\n   factory = WebSocketClientFactory(\"ws://localhost:9000\", debug = False)\\n   factory.protocol = MyClientProtocol\\n\\n   loop = asyncio.get_event_loop()\\n   coro = loop.create_connection(factory, \\'127.0.0.1\\', 9000)\\n   loop.run_until_complete(coro)\\n   loop.run_forever()\\n   loop.close()\\n', 'license': 'apache-2.0'}\n"
     ]
    }
   ],
   "source": [
    "iterator2 = iter(remote_dataset)\n",
    "print(next(iterator2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa0a37",
   "metadata": {},
   "source": [
    "## 10.1.4 허깅페이스 허브에 데이터셋 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c2b25",
   "metadata": {},
   "source": [
    "### 데이터셋을 허깅페이스 허브에 업로드하면 다음 작업이 가능해진다.  \n",
    "1. 훈련 서버에서 쉽게 다운로드한다.  \n",
    "2. 스트리밍 데이터셋을 허브 데이터셋과 함께 사용한다.  \n",
    "3. 해당 책의 독자는 물론이고 커뮤니티와 공유한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77677b",
   "metadata": {},
   "source": [
    "### 데이터셋을 업로드하려면 먼저 터미널에서 다음 명령을 실행하고 인증 정보를 입력해 허깅페이스 계정에 로그인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ad24df",
   "metadata": {},
   "source": [
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899070f",
   "metadata": {},
   "source": [
    "로그인한 후, 허브에 새로운 데이터셋을 만들고 압축된 JSON 파일을 직접 업로드한다.  \n",
    "작업을 간단하게 하기 위해 저장소를 두 개 만들기 (하나는 훈련 분할, 다른 하나는 검증 분할을 위한 것)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8952c88",
   "metadata": {},
   "source": [
    "### 터미널에 repo create 명령을 실행  \n",
    "~$ huggingface-cli repo create your_dataset_name --type dataset  \n",
    "  \n",
    "~$ huggingface-cli repo create your_dataset_name --type dataset --organization your-org-name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d7652",
   "metadata": {},
   "source": [
    "### 나는 이렇게 입력함\n",
    "~$ huggingface-cli repo create codeparrot-train --type dataset  \n",
    "  \n",
    "~$ huggingface-cli repo create codeparrot-valid --type dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf80045",
   "metadata": {},
   "source": [
    "여기서 저장소 타입을 (가중치를 저장하는 데 사용할 모델 저장소 대신) 데이터셋으로 지정한다.  \n",
    "또 이 저장소가 속할 조직을 지정한다.  \n",
    "개인 계정에서 이 명령을 실행한다면 --organization 옵션을 빼도 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63fb62",
   "metadata": {},
   "source": [
    "이제 비어 있는 두 저장소를 로컬 컴퓨터에 클론하고, JSON 파일을 각 저장소에 복사하고, 변경 사항을 허브에 푸시해야 한다.  \n",
    "184개 파일 중 마지막 JSON 압축 파일을 검증 파일로 선택하겠다. (즉, 전체 데이터셋의 약 0.5%다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfae0c9",
   "metadata": {},
   "source": [
    "### 다음 명령을 실행해 허브에서 로컬 컴퓨터로 저장소를 클론한다.\n",
    "Make sure you have git-lfs installed  \n",
    "https://git-lfs.github.com/  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc9ff9",
   "metadata": {},
   "source": [
    "~$ git lfs install (안되어 있다면)  \n",
    "  \n",
    "~$ git clone https://huggingface.co/datasets/namespace/your_dataset_name  \n",
    "(Here the namespace is either your username or your organization name.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea0f88",
   "metadata": {},
   "source": [
    "### 나는 이렇게\n",
    "~$ git clone https://huggingface.co/datasets/bh8648/codeparrot-train  \n",
    "\n",
    "~$ git clone https://huggingface.co/datasets/bh8648/codeparrot-valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40876b",
   "metadata": {},
   "source": [
    "### 마지막 깃허브 파일을 제외한 모든 파일을 훈련 세트로 복사한다.\n",
    "\n",
    "~$ cd codeparrot-train  \n",
    "\n",
    "~$ sudo cp /mnt/hdd_repo1/codeparrot/*.json.gz .  \n",
    "\n",
    "~$ rm ./file-000000000183.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77757c8",
   "metadata": {},
   "source": [
    "### 그다음 파일을 커밋하고 허브에 푸시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed233348",
   "metadata": {},
   "source": [
    "~$ git add .  \n",
    "\n",
    "~$ git commit -m \"Adding dataset files\"  \n",
    "\n",
    "~$ git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed037d",
   "metadata": {},
   "source": [
    "### validation은 마지막 샘플만 선택하여 위와 똑같이 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a18da",
   "metadata": {},
   "source": [
    "# 10.2 토크나이저 구축하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e4ba4",
   "metadata": {},
   "source": [
    "사전 훈련된 모델을 사용 -> 사전 훈련을 위해 선택한 전처리 방식을 동일하게 고수 (그렇지 않으면 분포를 벗어난 패턴이나 알 수 없는 토큰이 모델에 입력됨)   \n",
    "  \n",
    "새로운 모델을 훈련 -> 기존 토크나이저를 사용하는게 최적이 아닐 수 있음(기존 토크나이저가 훈련한 말뭉치 데이터셋이 광범위한 '불용어 필터링'을 적용했다던가, 프랑스어 텍스트로 훈련하여 평범한 영단어를 인식 못하는 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daa1ab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jj/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tok_list(tokenizer, string):\n",
    "    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n",
    "    return [tokenizer.decode(tok) for tok in input_ids]\n",
    "\n",
    "tokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc06b1",
   "metadata": {},
   "source": [
    "T5 토크나이저는 불용어 필터링이 광범위하게 적용되어 'sex' 같은 평범한 단어를 본 적이 없음  \n",
    "CamemBERT 토크나이저는 프랑스어 텍스트로만 훈련되어 'being'같은 평범한 영단어를 인식 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9c726f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"sex\"에 대한 T5 토큰: ['', 's', 'ex']\n",
      "\"being\"에 대한 CamemBERT 토큰: ['be', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(f'\"sex\"에 대한 T5 토큰: {tok_list(tokenizer_T5, \"sex\")}')\n",
    "print(f'\"being\"에 대한 CamemBERT 토큰: {tok_list(tokenizer_camembert, \"being\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e05db44",
   "metadata": {},
   "source": [
    "이렇게 짧고 평범한 단어를 부분으로 나누면 (문맥 길이가 제한된) 모델에 입력되는 시퀀스 길이가 늘어나서 비효율적이게 된다.  \n",
    "  \n",
    "그래서 토크나이저를 훈련하는 데 사용한 데이터셋의 도메인과 전처리 방식을 이해하는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f27cb",
   "metadata": {},
   "source": [
    "### 토크나이저와 모델은 데이터셋의 편향을 인코딩할 수 있는데, 이는 모델의 후속 행동에 영향을 미친다.\n",
    "따라서 데이터셋에 맞는 최적의 토크나이저를 얻으려면 토크나이저를 직접 훈련해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fdf06e",
   "metadata": {},
   "source": [
    "### ※ 모델의 훈련은 특정 목적하에 작업을 수행할 최적의 모델 가중치 집합을 찾는 과정이다. 그러나 토크나이저 훈련은 역전파나 가중치와 무관하다.\n",
    "토크나이저 훈련은 텍스트 문자열을 정수 리스트로 매핑하고 모델에 주입할 최적의 매핑을 찾는다.  \n",
    "  \n",
    "오늘날 토크나이저에서 최적의 문자열-정수 변환에는, 단위(atomic) 문자열의 리스트로 구성된 어휘사전과 변환, 정규화, 잘라내기, 텍스트 문자열을 인덱스 리스트로 매핑하기 등을 위한 메서드가 사용된다.  \n",
    "그다음 이 인덱스 리스트가 신경망의 입력이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2796d6",
   "metadata": {},
   "source": [
    "## 10.2.1 토크나이저 모델\n",
    "4장) 토크나이저는 정규화, 사전 토큰화, 토크나이저 모델, 사후 처리 네 단계로 구성된 전처리 파이프라인이다.  \n",
    "데이터로 훈련하는 토크나이저 파이프라인 부분이 토크나이저 모델이다.  \n",
    "2장) BPE, WordPeice, 유니그램 같은 부분단어 토큰화 알고리즘을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b4eee4",
   "metadata": {},
   "source": [
    "#### BPE는 기본 단위(단일 문자)의 리스트로 시작해서 가장 자주 함게 등장한 기본 단위를 합쳐 어휘사전에 추가하는 식으로 점진적으로 새 토큰을 만드는 과정을 거쳐 어휘사전을 만든다.  \n",
    "이 과정은 사전에 정의된 어휘사전 크기에 도달할 때까지 반복된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3be1b5",
   "metadata": {},
   "source": [
    "#### 유니그램은 반대로 먼저 말뭉치에 있는 모든 단어와 가능성 있는 부분단어로 기본 어휘사전을 구성한다.  \n",
    "그다음 어휘사전의 목표 크기에 도달할 때까지 점진적으로 유용성이 떨어지는 토큰을 삭제하거나 분할해 더 작은 어휘사전을 얻는다.  \n",
    "WordPiece는 유니그램의 전신이며 공식 구현은 구글에서 오픈소스로 공개한 적이 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3efbe",
   "metadata": {},
   "source": [
    "이런 다양한 알고리즘이 후속 작업의 성능에 미치는 영향은 그 작업에 따라 다르다.  \n",
    "#### BPE와 유니그램은 대부분의 경우 성능이 꽤 괜찮지만 평가할 때 고려할 점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd3744",
   "metadata": {},
   "source": [
    "## 10.2.2 토크나이저 성능 측정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb31c6a",
   "metadata": {},
   "source": [
    "1. 부분단어 생산력(subword fertility): 토큰화된 단어마다 생성되는 부분단어의 평균 개수를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f744b80",
   "metadata": {},
   "source": [
    "2. 연속 단어의 비율(proportion of continued words): 말뭉치에서 적어도 두 개의 부분 토큰으로 분할된 토큰화된 단어의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f6590",
   "metadata": {},
   "source": [
    "3. 커버리지 측정값(coverage metrics): 토큰화된 말뭉치에서 알 수 없는 단어나 거의 사용되지 않는 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616784b",
   "metadata": {},
   "source": [
    "때로는 철자 오류나 잡음에 대한 견고성, 도메인 밖 샘플에 대한 모델 성능을 추정하기도 한다.  \n",
    "(이런 성능은 토큰화 과정의 영향을 크게 받기 때문)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4a3c4",
   "metadata": {},
   "source": [
    "#### 이런 측정값들은 토큰화 성능에 대한 다양한 정보를 제공하지만 토크나이저와 모델의 상호작용을 간과하는 경향이 있다.\n",
    "예를 들어, 부분단어 생산력은 어휘사전에 가능한 모든 단어를 포함시켜 최소화할 수 있지만, 그러면 모델 입장에서 어휘사전이 매우 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc59b4",
   "metadata": {},
   "source": [
    "### 결국 다양한 토큰화 방식의 성능은 일반적으로 후속 모델의 성능을 궁극적인 지표로 사용할 때 가장 잘 추정된다.\n",
    "예를 들어, 초기 BPE 방식의 성능이 우수하다는 사실을 입증하기 위해 문자나 단어 기반 토큰화 대신 BPE 방식의 토크나이저와 어휘사전을 사용해 훈련한 모델이 기계 번역 작업의 성능을 향상시킴을 보였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c4ab8a",
   "metadata": {},
   "source": [
    "## 10.2.3 파이썬 코드를 위한 토크나이저"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc52e7",
   "metadata": {},
   "source": [
    "#### 코드 토큰화에 자연어 처리 토크나이저를 사용하는 것이 최적은 아닐 것 같음.  \n",
    "(공백에 중요한 의미가 있고 줄바꿈에는 별 의미가 없음, 밑줄 문자(_)로 여러 단어를 조합해 변수 이름을 만드는 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b271e",
   "metadata": {},
   "source": [
    "#### 허깅페이스 허브에서 이 작업에 유용한 토크나이저를 제공하는지 알아보자.  \n",
    "공백을 유지하는 토크나이저가 필요하니, GPT-2의 토크나이저와 같은 바이트 수준 토크나이저가 좋은 후보가 될 수 있다.  \n",
    "이 토크나이저를 로드하고 토큰화 방식을 살펴보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef5823",
   "metadata": {},
   "source": [
    "#### ※ 파이썬은 파이썬 코드를 의미 있는 단위(코드 연산, 주석, 들여쓰기, 내어쓰기 등)로 분할하는 tokenize 모듈을 내장하고 있으나 이 토크나이저가 파이썬 기반이라 보통 느리고 파이썬의 GIL(Golobal Interpreter Lock) 때문에 성능이 제한된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6cd8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "python_code = r\"\"\"def say_hello():\n",
    "    print(\"Hello, World!\")\n",
    "# Print it\n",
    "say_hello()\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d46055",
   "metadata": {},
   "source": [
    "출력이 상당히 이상하다. 이 토크나이저 파이프라인에 있는 다양한 서브 모듈을 실행해서 어떤작업을 수행하는지 알아보자.  \n",
    "### 먼저 토크나이저에 어떤 정규화가 적용됐는지 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bad2fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c69fe1",
   "metadata": {},
   "source": [
    "GPT-2 토크나이저는 정규화를 사용하지 않는다.  \n",
    "어떤 정규화 단계도 거치지 않고 원시 유니코드 입력을 바로 사용함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720de03",
   "metadata": {},
   "source": [
    "### 다음으로 사전 토큰화를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f2f2a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():', (13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('Hello', (28, 33)), (',', (33, 34)), ('ĠWorld', (34, 40)), ('!\")', (40, 43)), ('Ċ', (43, 44)), ('#', (44, 45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('say', (55, 58)), ('_', (58, 59)), ('hello', (59, 64)), ('()', (64, 66)), ('Ċ', (66, 67))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63386cb7",
   "metadata": {},
   "source": [
    "#### 토큰과 함께 출력된 숫자\n",
    "-> 토크나이저는 문자열과 토큰 사이를 전환하는 데 매우 유용한 오프셋 트랭킹(offset tracking)기능이 있다.  \n",
    "  \n",
    "-> 입력 문자열에 대한 모든 연산이 추적되기 때문에 토큰화 이후에 토큰이 입력 문자열의 어떤 부분에 해당하는지 정확하게 알 수 있다.  \n",
    "  \n",
    "-> 이 숫자는 단순히 토큰이 유래된 원본 문자열의 위치를 나타낸다.  \n",
    "  \n",
    "-> 예를 들어 첫째 줄의 단어 'hello'는 원본 문자열의 인덱스 8과 13 사이에 있다. 일부 문자가 정규화 단계에서 삭제되더라도 각 토큰을 원본 문자열의 해당 부분에 연결할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112c600",
   "metadata": {},
   "source": [
    "#### Ċ, Ġ  처럼 기이한 문자.\n",
    "-> 바이트 수준이란 표현은 이 토크나이저가 유니코드 문자가 아닌 바이트 단위로 동작함을 뜻한다.  \n",
    "  \n",
    "각 유니코드 문자는 문자에 따라 1에서 4바이트로 구성된다.  \n",
    "유니코드 알파벳에는 143,859개의 유니코드 문자가 있지만 바이트 알파벳에는 256개만 있다는 것이 바이트의 장점이다.  \n",
    "따라서 유니코드 문자를 이런 바이트의 시퀀스로 변환할 수 있다.  \n",
    "  \n",
    "만약 바이트를 사용하면 UTF-8로 구성된 어떤 문자열도 256개 값의 알파벳으로 구성된 더 긴 문자열로 표현 가능하다.  \n",
    "즉, 256개 알파벳만 사용해 어떤 유니코드 문자열도 처리하는 모델이 생긴다.  \n",
    "  \n",
    "일부 문자의 바이트 표현을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e68af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`a`는 단일 바이트 b'a'로 인코딩됩니다: 97\n",
      "`€`는 세 바이트 b'\\xe2\\x82\\xac'로 인코딩됩니다: [226, 130, 172]\n"
     ]
    }
   ],
   "source": [
    "a, e = u\"a\", u\"€\"\n",
    "byte = ord(a.encode(\"utf-8\"))\n",
    "print(f'`{a}`는 단일 바이트 {a.encode(\"utf-8\")}로 인코딩됩니다: {byte}')\n",
    "byte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\n",
    "print(f'`{e}`는 세 바이트 {e.encode(\"utf-8\")}로 인코딩됩니다: {byte}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477803e",
   "metadata": {},
   "source": [
    "gpt-2는 256개 입력 바이트를 출력 가능한 표준 유니코드 문자에 해당하는 유니코드 문자열로 매핑한 다음 BPE 알고리즘을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6526754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 어휘 사전 크기: 256\n",
      "첫 번째 원소: `!`, last element: `Ń`\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "print(f'기본 어휘 사전 크기: {len(base_vocab)}')\n",
    "print(f'첫 번째 원소: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061352de",
   "metadata": {},
   "source": [
    "#### ※ NLP의 표준 Byte-Pair Encoding(BPE)은 이름과 달리 대개 유니코드 문자열에서 작동한다.  \n",
    "#### ※ Byte-Level(바이트 수준) BPE는 바이트에서 동작하는 신형 BPE다.  \n",
    "유니코드 문자열을 바이트로 읽는다면 간단한 BPE 부분단어 분할 알고리즘의 재사용이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "080bd461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Character</th>\n",
       "      <th>Bytes</th>\n",
       "      <th>Mapped bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regular characters</td>\n",
       "      <td>`a` and `?`</td>\n",
       "      <td>97 and 63</td>\n",
       "      <td>`a` and `?`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nonprintable control character (carriage return)</td>\n",
       "      <td>`U+000D`</td>\n",
       "      <td>13</td>\n",
       "      <td>`č`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A space</td>\n",
       "      <td>` `</td>\n",
       "      <td>32</td>\n",
       "      <td>`Ġ`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A nonbreakable space</td>\n",
       "      <td>`\\xa0`</td>\n",
       "      <td>160</td>\n",
       "      <td>`ł`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A newline character</td>\n",
       "      <td>`\\n`</td>\n",
       "      <td>10</td>\n",
       "      <td>`Ċ`</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Description    Character      Bytes  \\\n",
       "0                                Regular characters  `a` and `?`  97 and 63   \n",
       "1  Nonprintable control character (carriage return)     `U+000D`         13   \n",
       "2                                           A space          ` `         32   \n",
       "3                              A nonbreakable space       `\\xa0`        160   \n",
       "4                               A newline character         `\\n`         10   \n",
       "\n",
       "  Mapped bytes  \n",
       "0  `a` and `?`  \n",
       "1          `č`  \n",
       "2          `Ġ`  \n",
       "3          `ł`  \n",
       "4          `Ċ`  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPE 문자 매핑의 예\n",
    "import pandas as pd\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "examples = [\n",
    "    ['Regular characters', '`a` and `?`', f'{ord(\"a\")} and {ord(\"?\")}' , f'`{byte_to_unicode_map[ord(\"a\")]}` and `{byte_to_unicode_map[ord(\"?\")]}`'],\n",
    "    ['Nonprintable control character (carriage return)', '`U+000D`', f'13', f'`{byte_to_unicode_map[13]}`'],\n",
    "    ['A space', '` `', f'{ord(\" \")}', f'`{byte_to_unicode_map[ord(\" \")]}`'],\n",
    "    ['A nonbreakable space', '`\\\\xa0`', '160', f'`{byte_to_unicode_map[ord(chr(160))]}`'],\n",
    "    ['A newline character', '`\\\\n`', '10', f'`{byte_to_unicode_map[ord(chr(10))]}`'],\n",
    "]\n",
    "\n",
    "pd.DataFrame(examples, columns = ['Description', 'Character', 'Bytes', 'Mapped bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e2516d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():', (13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('Hello', (28, 33)), (',', (33, 34)), ('ĠWorld', (34, 40)), ('!\")', (40, 43)), ('Ċ', (43, 44)), ('#', (44, 45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('say', (55, 58)), ('_', (58, 59)), ('hello', (59, 64)), ('()', (64, 66)), ('Ċ', (66, 67))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d5bed",
   "metadata": {},
   "source": [
    "#### Ċ는 줄바꿈, Ġ는 공백을 표시한 것을 알 수 있다.\n",
    "#### 공백, 특히 연속된 공백이 보존된다. (예를 들어, ĊĠĠĠ는 공백 세 개를 의미한다)\n",
    "#### 연속된 공백은 단어 하나로 간주한다.\n",
    "#### 단어 앞 공백은 후속 단어에 포함된 단어의 일부로 간주한다. (가령 Ġsay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67b7e2",
   "metadata": {},
   "source": [
    "### 그럼 BPE 알고리즘으로 실험해보자. 이미 언급했듯 미리 정의된 어휘사전 크기에 도달할 때까지 단어를 부분으로 나눈다.\n",
    "GPT-2 토크나이저의 어휘사전은 50,257개 단어로 구성된다.  \n",
    "기본 어휘사전은 256개 바이트 값에 해당한다.  \n",
    "50,000개 추가 토큰은 가장 자주 함께 등장한 토큰을 반복적으로 합쳐 만든다.  \n",
    "문서 경계를 나타내는 특별한 문자가 어휘사전에 추가된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9122a",
   "metadata": {},
   "source": [
    "#### 이 내용을 토크나이저의 길이 속성에서 쉽게 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ce22a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘사전의 크기: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"어휘사전의 크기: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a952384",
   "metadata": {},
   "source": [
    "샘플 입력 코드에 전체 파이프라인을 실행하면 다음과 같은 결과를 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0b48df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e6fc8",
   "metadata": {},
   "source": [
    "BPE 토크나이저는 대부분 단어를 유지한다.  \n",
    "하지만 들여쓰기에 있는 여러 공백을 몇 개의 연속된 공백으로 나눈다.  \n",
    "(코드에서 훈련되지 않았기 때문, 대부분 텍스트에서 연속된 공백은 드물다.)  \n",
    "### 이러한 경우가 토크나이저 모델이 주어진 데이터셋의 도메인에 잘 맞지 않는 한 예다.\n",
    "### -> 앞서 논의했듯 타깃 말뭉치에서 토크나이저를 재훈련하는 것이 해결책이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135ec37",
   "metadata": {},
   "source": [
    "## 10.2.4 토크나이저 훈련하기\n",
    "이 예제의 말뭉치에서 바이트 수준 BPE 토크나이저를 재훈련하겠다.  \n",
    "#### 트랜스포머스가 제공하는 토크나이저는 간단하게 재훈련되는데, 다음 내용이 필요하다.\n",
    "1. 목표 어휘사전의 크기를 지정한다.  \n",
    "2. 토크나이저 모델을 훈련하기 위해 입력 문자열을 공급할 반복자(iterator)를 준비한다.  \n",
    "3. train_new_from_iterator() 메서드를 호출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d67093",
   "metadata": {},
   "source": [
    "#### 딥러닝 모델은 훈련 말뭉치에서 특정 세부 내용을 많이 기억하도록 훈련하지만,\n",
    "#### 토크나이저는 주요 통곗값을 추출하도록 훈련한다.  \n",
    "-> 간단히 말해 토크나이저는 말뭉치에서 가장 자주 등장한 문자 조합을 알아내는 훈련을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f7f2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1ebb4ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ\n",
      "\n",
      "Ġ=================================================================\n",
      "\n",
      "Ġ----------------------------------------------------------------\n",
      "\n",
      "================================================================\n",
      "\n",
      "ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ\n",
      "\n",
      "________________________________________________________________\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "................................................................\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t, _ in tokens[:8]:\n",
    "    print(t+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cdbbe9",
   "metadata": {},
   "source": [
    "#### 위는 온라인 포럼에서 사용하는 구분선으로 보인다.  \n",
    "#### 이번에는 가장 드물게 나타나 어휘사전의 마지막에 등록된 단어를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61fad6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'Ġgazed', 'Ġinformants', 'ĠCollider', 'Ġregress', 'ominated', 'Ġamplification', 'Compar', 'âĢ¦.\"', 'Ġ(/', 'Commission', 'ĠHitman']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print([f'{tokenizer.convert_ids_to_tokens(i)}' for t, i in tokens[:12]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b13eb2",
   "metadata": {},
   "source": [
    "### 첫 토큰 <|endoftext|>은 텍스트 시퀀스의 끝을 지정할 때 사용하는 특수 토큰으로, BPE 어휘사전이 구축된 후 추가된다.\n",
    "모델은 각 토큰에 관련된 단어 임베딩을 학습해야 하는데, 임베딩 행렬에 잡음 단어가 많이 포함되어 있다면 좋지 않을 것.  \n",
    "또한 세상의 시공간적 지식(가령 Hitman과 Commission 같은 고유 명사)을 어휘사전에 있는 벡터와 함께 별개의 토큰으로 부여해 저수준에서 이를 임베딩할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bfd2f",
   "metadata": {},
   "source": [
    "#### BPE 토크나이저가 이런 토큰을 생성한다면 목표 어휘사전이 너무 크거나 말뭉치에 특수 토큰이 포함됐다는 신호가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbde678",
   "metadata": {},
   "source": [
    "### 예제의 말뭉치에서 새 토크나이저를 훈련하고 학습된 어휘사전을 살펴보겠다.\n",
    "데이터셋의 통계를 대표할 말뭉치가 필요하니 말뭉치에서 약 100,000개 문서 (1~2GB 데이터)를 선택하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0057e364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '¡',\n",
       " '¢',\n",
       " '£',\n",
       " '¤',\n",
       " '¥',\n",
       " '¦',\n",
       " '§',\n",
       " '¨',\n",
       " '©',\n",
       " 'ª',\n",
       " '«',\n",
       " '¬',\n",
       " '®',\n",
       " '¯',\n",
       " '°',\n",
       " '±',\n",
       " '²',\n",
       " '³',\n",
       " '´',\n",
       " 'µ',\n",
       " '¶',\n",
       " '·',\n",
       " '¸',\n",
       " '¹',\n",
       " 'º',\n",
       " '»',\n",
       " '¼',\n",
       " '½',\n",
       " '¾',\n",
       " '¿',\n",
       " 'À',\n",
       " 'Á',\n",
       " 'Â',\n",
       " 'Ã',\n",
       " 'Ä',\n",
       " 'Å',\n",
       " 'Æ',\n",
       " 'Ç',\n",
       " 'È',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'Ë',\n",
       " 'Ì',\n",
       " 'Í',\n",
       " 'Î',\n",
       " 'Ï',\n",
       " 'Ð',\n",
       " 'Ñ',\n",
       " 'Ò',\n",
       " 'Ó',\n",
       " 'Ô',\n",
       " 'Õ',\n",
       " 'Ö',\n",
       " '×',\n",
       " 'Ø',\n",
       " 'Ù',\n",
       " 'Ú',\n",
       " 'Û',\n",
       " 'Ü',\n",
       " 'Ý',\n",
       " 'Þ',\n",
       " 'ß',\n",
       " 'à',\n",
       " 'á',\n",
       " 'â',\n",
       " 'ã',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'æ',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'ë',\n",
       " 'ì',\n",
       " 'í',\n",
       " 'î',\n",
       " 'ï',\n",
       " 'ð',\n",
       " 'ñ',\n",
       " 'ò',\n",
       " 'ó',\n",
       " 'ô',\n",
       " 'õ',\n",
       " 'ö',\n",
       " '÷',\n",
       " 'ø',\n",
       " 'ù',\n",
       " 'ú',\n",
       " 'û',\n",
       " 'ü',\n",
       " 'ý',\n",
       " 'þ',\n",
       " 'ÿ',\n",
       " 'Ā',\n",
       " 'ā',\n",
       " 'Ă',\n",
       " 'ă',\n",
       " 'Ą',\n",
       " 'ą',\n",
       " 'Ć',\n",
       " 'ć',\n",
       " 'Ĉ',\n",
       " 'ĉ',\n",
       " 'Ċ',\n",
       " 'ċ',\n",
       " 'Č',\n",
       " 'č',\n",
       " 'Ď',\n",
       " 'ď',\n",
       " 'Đ',\n",
       " 'đ',\n",
       " 'Ē',\n",
       " 'ē',\n",
       " 'Ĕ',\n",
       " 'ĕ',\n",
       " 'Ė',\n",
       " 'ė',\n",
       " 'Ę',\n",
       " 'ę',\n",
       " 'Ě',\n",
       " 'ě',\n",
       " 'Ĝ',\n",
       " 'ĝ',\n",
       " 'Ğ',\n",
       " 'ğ',\n",
       " 'Ġ',\n",
       " 'ġ',\n",
       " 'Ģ',\n",
       " 'ģ',\n",
       " 'Ĥ',\n",
       " 'ĥ',\n",
       " 'Ħ',\n",
       " 'ħ',\n",
       " 'Ĩ',\n",
       " 'ĩ',\n",
       " 'Ī',\n",
       " 'ī',\n",
       " 'Ĭ',\n",
       " 'ĭ',\n",
       " 'Į',\n",
       " 'į',\n",
       " 'İ',\n",
       " 'ı',\n",
       " 'Ĳ',\n",
       " 'ĳ',\n",
       " 'Ĵ',\n",
       " 'ĵ',\n",
       " 'Ķ',\n",
       " 'ķ',\n",
       " 'ĸ',\n",
       " 'Ĺ',\n",
       " 'ĺ',\n",
       " 'Ļ',\n",
       " 'ļ',\n",
       " 'Ľ',\n",
       " 'ľ',\n",
       " 'Ŀ',\n",
       " 'ŀ',\n",
       " 'Ł',\n",
       " 'ł',\n",
       " 'Ń']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fffc71f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999907e0",
   "metadata": {},
   "source": [
    "### 시간 좀 오래 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1294f975",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d228f8ebcd4d95ae4df38ebbc8efe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33da3c4136b4b3e8992bd8298be90e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "length = 100000\n",
    "dataset_name = 'transformersbook/codeparrot-train'\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "iter_dataset = iter(dataset)\n",
    "\n",
    "def batch_iterator(batch_size=10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
    "\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), \n",
    "                                                  vocab_size=12500,\n",
    "                                                  initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f145a3",
   "metadata": {},
   "source": [
    "AutoTokenizer.train_new_from_iterator()는 사용 중인 토크나이저가 \"빠른(fast)\" 토크나이저인 경우에만 작동합니다. 다음 섹션에서 볼 수 있듯이 🤗Transformers 라이브러리에는 두 가지 유형의 토크나이저가 포함되어 있습니다. 한 유형은 순수하게 Python으로 작성되어 있고 다른 유형(빠른 토크나이저)은 🤗Tokenizers 라이브러리의 도움을 받아서 Rust 프로그래밍 언어로 작성된 토크나이저입니다.  \n",
    "  \n",
    "트랜스포머스에는 기존에 존재하는 것들과 동일한 특성을 가진 새로운 토크나이저를 학습하는데 사용할 수 있는 매우 간단한 API가 있습니다. 바로 AutoTokenizer.train_new_from_iterator()가 그것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50597f",
   "metadata": {},
   "source": [
    "### 이 어휘사전이 작업과 얼마나 관련됐는지 알아보기 위해 BPE 알고리즘이 만든 첫 단어와 마지막 단어를 조사해보자.\n",
    "256바이트는 건너뛰고 그 다음에 추가된 첫 토큰을 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0410bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1be716ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', 0),\n",
       " ('!', 1),\n",
       " ('\"', 2),\n",
       " ('#', 3),\n",
       " ('$', 4),\n",
       " ('%', 5),\n",
       " ('&', 6),\n",
       " (\"'\", 7),\n",
       " ('(', 8),\n",
       " (')', 9),\n",
       " ('*', 10),\n",
       " ('+', 11),\n",
       " (',', 12),\n",
       " ('-', 13),\n",
       " ('.', 14),\n",
       " ('/', 15),\n",
       " ('0', 16),\n",
       " ('1', 17),\n",
       " ('2', 18),\n",
       " ('3', 19),\n",
       " ('4', 20),\n",
       " ('5', 21),\n",
       " ('6', 22),\n",
       " ('7', 23),\n",
       " ('8', 24),\n",
       " ('9', 25),\n",
       " (':', 26),\n",
       " (';', 27),\n",
       " ('<', 28),\n",
       " ('=', 29),\n",
       " ('>', 30),\n",
       " ('?', 31),\n",
       " ('@', 32),\n",
       " ('A', 33),\n",
       " ('B', 34),\n",
       " ('C', 35),\n",
       " ('D', 36),\n",
       " ('E', 37),\n",
       " ('F', 38),\n",
       " ('G', 39),\n",
       " ('H', 40),\n",
       " ('I', 41),\n",
       " ('J', 42),\n",
       " ('K', 43),\n",
       " ('L', 44),\n",
       " ('M', 45),\n",
       " ('N', 46),\n",
       " ('O', 47),\n",
       " ('P', 48),\n",
       " ('Q', 49),\n",
       " ('R', 50),\n",
       " ('S', 51),\n",
       " ('T', 52),\n",
       " ('U', 53),\n",
       " ('V', 54),\n",
       " ('W', 55),\n",
       " ('X', 56),\n",
       " ('Y', 57),\n",
       " ('Z', 58),\n",
       " ('[', 59),\n",
       " ('\\\\', 60),\n",
       " (']', 61),\n",
       " ('^', 62),\n",
       " ('_', 63),\n",
       " ('`', 64),\n",
       " ('a', 65),\n",
       " ('b', 66),\n",
       " ('c', 67),\n",
       " ('d', 68),\n",
       " ('e', 69),\n",
       " ('f', 70),\n",
       " ('g', 71),\n",
       " ('h', 72),\n",
       " ('i', 73),\n",
       " ('j', 74),\n",
       " ('k', 75),\n",
       " ('l', 76),\n",
       " ('m', 77),\n",
       " ('n', 78),\n",
       " ('o', 79),\n",
       " ('p', 80),\n",
       " ('q', 81),\n",
       " ('r', 82),\n",
       " ('s', 83),\n",
       " ('t', 84),\n",
       " ('u', 85),\n",
       " ('v', 86),\n",
       " ('w', 87),\n",
       " ('x', 88),\n",
       " ('y', 89),\n",
       " ('z', 90),\n",
       " ('{', 91),\n",
       " ('|', 92),\n",
       " ('}', 93),\n",
       " ('~', 94),\n",
       " ('¡', 95),\n",
       " ('¢', 96),\n",
       " ('£', 97),\n",
       " ('¤', 98),\n",
       " ('¥', 99),\n",
       " ('¦', 100),\n",
       " ('§', 101),\n",
       " ('¨', 102),\n",
       " ('©', 103),\n",
       " ('ª', 104),\n",
       " ('«', 105),\n",
       " ('¬', 106),\n",
       " ('®', 107),\n",
       " ('¯', 108),\n",
       " ('°', 109),\n",
       " ('±', 110),\n",
       " ('²', 111),\n",
       " ('³', 112),\n",
       " ('´', 113),\n",
       " ('µ', 114),\n",
       " ('¶', 115),\n",
       " ('·', 116),\n",
       " ('¸', 117),\n",
       " ('¹', 118),\n",
       " ('º', 119),\n",
       " ('»', 120),\n",
       " ('¼', 121),\n",
       " ('½', 122),\n",
       " ('¾', 123),\n",
       " ('¿', 124),\n",
       " ('À', 125),\n",
       " ('Á', 126),\n",
       " ('Â', 127),\n",
       " ('Ã', 128),\n",
       " ('Ä', 129),\n",
       " ('Å', 130),\n",
       " ('Æ', 131),\n",
       " ('Ç', 132),\n",
       " ('È', 133),\n",
       " ('É', 134),\n",
       " ('Ê', 135),\n",
       " ('Ë', 136),\n",
       " ('Ì', 137),\n",
       " ('Í', 138),\n",
       " ('Î', 139),\n",
       " ('Ï', 140),\n",
       " ('Ð', 141),\n",
       " ('Ñ', 142),\n",
       " ('Ò', 143),\n",
       " ('Ó', 144),\n",
       " ('Ô', 145),\n",
       " ('Õ', 146),\n",
       " ('Ö', 147),\n",
       " ('×', 148),\n",
       " ('Ø', 149),\n",
       " ('Ù', 150),\n",
       " ('Ú', 151),\n",
       " ('Û', 152),\n",
       " ('Ü', 153),\n",
       " ('Ý', 154),\n",
       " ('Þ', 155),\n",
       " ('ß', 156),\n",
       " ('à', 157),\n",
       " ('á', 158),\n",
       " ('â', 159),\n",
       " ('ã', 160),\n",
       " ('ä', 161),\n",
       " ('å', 162),\n",
       " ('æ', 163),\n",
       " ('ç', 164),\n",
       " ('è', 165),\n",
       " ('é', 166),\n",
       " ('ê', 167),\n",
       " ('ë', 168),\n",
       " ('ì', 169),\n",
       " ('í', 170),\n",
       " ('î', 171),\n",
       " ('ï', 172),\n",
       " ('ð', 173),\n",
       " ('ñ', 174),\n",
       " ('ò', 175),\n",
       " ('ó', 176),\n",
       " ('ô', 177),\n",
       " ('õ', 178),\n",
       " ('ö', 179),\n",
       " ('÷', 180),\n",
       " ('ø', 181),\n",
       " ('ù', 182),\n",
       " ('ú', 183),\n",
       " ('û', 184),\n",
       " ('ü', 185),\n",
       " ('ý', 186),\n",
       " ('þ', 187),\n",
       " ('ÿ', 188),\n",
       " ('Ā', 189),\n",
       " ('ā', 190),\n",
       " ('Ă', 191),\n",
       " ('ă', 192),\n",
       " ('Ą', 193),\n",
       " ('ą', 194),\n",
       " ('Ć', 195),\n",
       " ('ć', 196),\n",
       " ('Ĉ', 197),\n",
       " ('ĉ', 198),\n",
       " ('Ċ', 199),\n",
       " ('ċ', 200),\n",
       " ('Č', 201),\n",
       " ('č', 202),\n",
       " ('Ď', 203),\n",
       " ('ď', 204),\n",
       " ('Đ', 205),\n",
       " ('đ', 206),\n",
       " ('Ē', 207),\n",
       " ('ē', 208),\n",
       " ('Ĕ', 209),\n",
       " ('ĕ', 210),\n",
       " ('Ė', 211),\n",
       " ('ė', 212),\n",
       " ('Ę', 213),\n",
       " ('ę', 214),\n",
       " ('Ě', 215),\n",
       " ('ě', 216),\n",
       " ('Ĝ', 217),\n",
       " ('ĝ', 218),\n",
       " ('Ğ', 219),\n",
       " ('ğ', 220),\n",
       " ('Ġ', 221),\n",
       " ('ġ', 222),\n",
       " ('Ģ', 223),\n",
       " ('ģ', 224),\n",
       " ('Ĥ', 225),\n",
       " ('ĥ', 226),\n",
       " ('Ħ', 227),\n",
       " ('ħ', 228),\n",
       " ('Ĩ', 229),\n",
       " ('ĩ', 230),\n",
       " ('Ī', 231),\n",
       " ('ī', 232),\n",
       " ('Ĭ', 233),\n",
       " ('ĭ', 234),\n",
       " ('Į', 235),\n",
       " ('į', 236),\n",
       " ('İ', 237),\n",
       " ('ı', 238),\n",
       " ('Ĳ', 239),\n",
       " ('ĳ', 240),\n",
       " ('Ĵ', 241),\n",
       " ('ĵ', 242),\n",
       " ('Ķ', 243),\n",
       " ('ķ', 244),\n",
       " ('ĸ', 245),\n",
       " ('Ĺ', 246),\n",
       " ('ĺ', 247),\n",
       " ('Ļ', 248),\n",
       " ('ļ', 249),\n",
       " ('Ľ', 250),\n",
       " ('ľ', 251),\n",
       " ('Ŀ', 252),\n",
       " ('ŀ', 253),\n",
       " ('Ł', 254),\n",
       " ('ł', 255),\n",
       " ('Ń', 256),\n",
       " ('ĠĠ', 257),\n",
       " ('ĠĠĠĠ', 258),\n",
       " ('ĠĠĠ', 259),\n",
       " ('ĠĠĠĠĠĠĠĠ', 260),\n",
       " ('se', 261),\n",
       " ('in', 262),\n",
       " ('ĠĠĠĠĠĠĠ', 263),\n",
       " ('re', 264),\n",
       " ('on', 265),\n",
       " ('te', 266),\n",
       " ('ĊĠĠĠĠĠĠĠ', 267),\n",
       " ('ĊĠĠĠĠĠĠĠĠ', 268),\n",
       " ('or', 269),\n",
       " ('st', 270),\n",
       " ('de', 271),\n",
       " ('ĊĠĠĠ', 272),\n",
       " ('th', 273),\n",
       " ('le', 274),\n",
       " ('Ġ=', 275),\n",
       " ('lf', 276),\n",
       " ('self', 277),\n",
       " ('me', 278),\n",
       " ('al', 279),\n",
       " ('ti', 280),\n",
       " ('er', 281),\n",
       " ('Ġa', 282),\n",
       " (\"Ġ'\", 283),\n",
       " ('Ġi', 284),\n",
       " ('ar', 285),\n",
       " ('Ġc', 286),\n",
       " ('en', 287),\n",
       " ('Ġf', 288),\n",
       " ('ĊĠĠĠĠĠĠĠĠĠĠĠ', 289),\n",
       " ('an', 290),\n",
       " ('Ġself', 291),\n",
       " ('at', 292),\n",
       " ('Ġth', 293),\n",
       " ('ro', 294),\n",
       " ('Ġre', 295),\n",
       " ('tion', 296),\n",
       " ('Ġp', 297),\n",
       " ('ur', 298),\n",
       " ('Ġ\"', 299),\n",
       " ('ce', 300),\n",
       " (\"',\", 301),\n",
       " ('Ġn', 302),\n",
       " ('ge', 303),\n",
       " ('--', 304),\n",
       " ('):', 305),\n",
       " ('as', 306),\n",
       " ('Ġt', 307),\n",
       " ('Ġs', 308),\n",
       " ('##', 309),\n",
       " ('mp', 310),\n",
       " ('ue', 311),\n",
       " ('Ġo', 312),\n",
       " ('ame', 313),\n",
       " ('Ġthe', 314),\n",
       " ('Ġin', 315),\n",
       " ('ing', 316),\n",
       " ('li', 317),\n",
       " ('def', 318),\n",
       " ('ct', 319),\n",
       " ('lo', 320),\n",
       " ('ri', 321),\n",
       " ('pe', 322),\n",
       " ('ate', 323),\n",
       " ('un', 324),\n",
       " ('Ġe', 325),\n",
       " ('di', 326),\n",
       " ('Ġ#', 327),\n",
       " ('ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 328),\n",
       " ('Ġb', 329),\n",
       " ('fi', 330),\n",
       " ('co', 331),\n",
       " ('ser', 332),\n",
       " ('Ġm', 333),\n",
       " ('Ġ(', 334),\n",
       " ('Ġw', 335),\n",
       " ('ch', 336),\n",
       " ('ut', 337),\n",
       " ('si', 338),\n",
       " ('ĊĊĠĠĠ', 339),\n",
       " ('Ġif', 340),\n",
       " ('\"\"', 341),\n",
       " ('nt', 342),\n",
       " ('()', 343),\n",
       " ('ra', 344),\n",
       " ('id', 345),\n",
       " ('Ġdef', 346),\n",
       " ('ck', 347),\n",
       " ('urn', 348),\n",
       " ('ul', 349),\n",
       " ('turn', 350),\n",
       " ('el', 351),\n",
       " ('ter', 352),\n",
       " ('ad', 353),\n",
       " ('name', 354),\n",
       " (\"':\", 355),\n",
       " ('ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 356),\n",
       " ('ot', 357),\n",
       " ('Ġ[', 358),\n",
       " (\"')\", 359),\n",
       " ('ort', 360),\n",
       " (\"('\", 361),\n",
       " ('get', 362),\n",
       " ('__', 363),\n",
       " ('Ġis', 364),\n",
       " ('od', 365),\n",
       " ('Ġfor', 366),\n",
       " ('one', 367),\n",
       " ('ty', 368),\n",
       " ('Ġto', 369),\n",
       " ('Ġd', 370),\n",
       " ('----', 371),\n",
       " ('ab', 372),\n",
       " ('Ġreturn', 373),\n",
       " ('is', 374),\n",
       " ('Ġv', 375),\n",
       " ('Ġan', 376),\n",
       " ('ĠT', 377),\n",
       " ('ed', 378),\n",
       " ('Ġ0', 379),\n",
       " ('####', 380),\n",
       " ('ode', 381),\n",
       " ('00', 382),\n",
       " ('Ġu', 383),\n",
       " ('il', 384),\n",
       " ('ss', 385),\n",
       " ('pa', 386),\n",
       " ('et', 387),\n",
       " ('con', 388),\n",
       " ('==', 389),\n",
       " ('ma', 390),\n",
       " ('it', 391),\n",
       " ('qu', 392),\n",
       " ('Ġh', 393),\n",
       " ('ol', 394),\n",
       " ('up', 395),\n",
       " ('es', 396),\n",
       " ('test', 397),\n",
       " ('),', 398),\n",
       " ('Ġof', 399),\n",
       " ('ĊĊĠĠĠĠĠĠĠ', 400),\n",
       " ('xt', 401),\n",
       " ('\",', 402),\n",
       " ('None', 403),\n",
       " ('ass', 404),\n",
       " ('alue', 405),\n",
       " ('sert', 406),\n",
       " ('Ġ\"\"\"', 407),\n",
       " ('op', 408),\n",
       " ('set', 409),\n",
       " ('Ġcon', 410),\n",
       " ('Ġst', 411),\n",
       " ('mport', 412),\n",
       " ('ke', 413),\n",
       " ('Ġ1', 414),\n",
       " ('la', 415),\n",
       " (\"']\", 416),\n",
       " ('ction', 417),\n",
       " ('rom', 418),\n",
       " ('ĠĠĠĠĠ', 419),\n",
       " ('ĊĊ', 420),\n",
       " ('ata', 421),\n",
       " ('._', 422),\n",
       " ('ver', 423),\n",
       " ('der', 424),\n",
       " ('he', 425),\n",
       " ('ult', 426),\n",
       " ('ation', 427),\n",
       " ('cl', 428),\n",
       " ('ĠS', 429),\n",
       " ('ve', 430),\n",
       " ('))', 431),\n",
       " ('rue', 432),\n",
       " ('vi', 433),\n",
       " ('Ġand', 434),\n",
       " ('ment', 435),\n",
       " ('sion', 436),\n",
       " ('ĠA', 437),\n",
       " ('Ġ+', 438),\n",
       " ('Ġnot', 439),\n",
       " ('ile', 440),\n",
       " ('ap', 441),\n",
       " ('int', 442),\n",
       " ('Ġ-', 443),\n",
       " ('ith', 444),\n",
       " ('ub', 445),\n",
       " ('ĠC', 446),\n",
       " ('Ġex', 447),\n",
       " ('ect', 448),\n",
       " ('all', 449),\n",
       " ('gs', 450),\n",
       " ('ror', 451),\n",
       " ('ĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 452),\n",
       " ('am', 453),\n",
       " ('eld', 454),\n",
       " ('Ġ%', 455),\n",
       " ('tr', 456),\n",
       " ('ule', 457),\n",
       " ('um', 458),\n",
       " (\"['\", 459),\n",
       " ('and', 460),\n",
       " ('par', 461),\n",
       " ('able', 462),\n",
       " ('::', 463),\n",
       " ('Ġas', 464),\n",
       " ('bu', 465),\n",
       " ('type', 466),\n",
       " ('],', 467),\n",
       " ('bj', 468),\n",
       " ('Ġ{', 469),\n",
       " ('ted', 470),\n",
       " ('res', 471),\n",
       " ('ĠI', 472),\n",
       " ('Ġde', 473),\n",
       " ('assert', 474),\n",
       " ('lin', 475),\n",
       " ('ex', 476),\n",
       " ('to', 477),\n",
       " ('age', 478),\n",
       " ('(\"', 479),\n",
       " ('ase', 480),\n",
       " ('ĠF', 481),\n",
       " ('ls', 482),\n",
       " ('Ġ_', 483),\n",
       " ('//', 484),\n",
       " ('Ġdi', 485),\n",
       " ('ent', 486),\n",
       " ('Ġg', 487),\n",
       " ('ĊĠĠĠĠĠ', 488),\n",
       " ('odule', 489),\n",
       " ('ĠNone', 490),\n",
       " ('file', 491),\n",
       " ('ĠL', 492),\n",
       " ('ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 493),\n",
       " ('Ġimport', 494),\n",
       " ('ption', 495),\n",
       " ('pro', 496),\n",
       " ('key', 497),\n",
       " ('end', 498),\n",
       " ('str', 499),\n",
       " ('Ġor', 500),\n",
       " ('Ġ2', 501),\n",
       " ('Re', 502),\n",
       " ('from', 503),\n",
       " ('Ċĉ', 504),\n",
       " ('ta', 505),\n",
       " ('Ġbe', 506),\n",
       " ('text', 507),\n",
       " ('ĠP', 508),\n",
       " ('Ġco', 509),\n",
       " ('for', 510),\n",
       " ('--------', 511),\n",
       " ('Ġ==', 512),\n",
       " ('path', 513),\n",
       " ('Ġ:', 514),\n",
       " ('list', 515),\n",
       " ('Ġtest', 516),\n",
       " ('ype', 517),\n",
       " ('########', 518),\n",
       " ('ĊĠ', 519),\n",
       " ('ht', 520),\n",
       " ('time', 521),\n",
       " ('Ġr', 522),\n",
       " ('Ġres', 523),\n",
       " ('put', 524),\n",
       " ('ho', 525),\n",
       " ('ig', 526),\n",
       " ('us', 527),\n",
       " ('**', 528),\n",
       " ('per', 529),\n",
       " ('\")', 530),\n",
       " ('add', 531),\n",
       " ('ic', 532),\n",
       " ('ER', 533),\n",
       " ('class', 534),\n",
       " ('qual', 535),\n",
       " ('Ġname', 536),\n",
       " ('====', 537),\n",
       " ('Er', 538),\n",
       " ('Ġse', 539),\n",
       " ('alse', 540),\n",
       " ('Ġwith', 541),\n",
       " ('pt', 542),\n",
       " ('que', 543),\n",
       " ('ew', 544),\n",
       " ('Error', 545),\n",
       " ('fo', 546),\n",
       " ('out', 547),\n",
       " ('ield', 548),\n",
       " ('True', 549),\n",
       " ('bject', 550),\n",
       " ('sc', 551),\n",
       " ('len', 552),\n",
       " ('ff', 553),\n",
       " ('ance', 554),\n",
       " ('po', 555),\n",
       " (\"='\", 556),\n",
       " ('Ġpro', 557),\n",
       " ('ault', 558),\n",
       " ('ci', 559),\n",
       " ('Ġlo', 560),\n",
       " ('tem', 561),\n",
       " ('ze', 562),\n",
       " ('Ġme', 563),\n",
       " ('ack', 564),\n",
       " ('])', 565),\n",
       " ('our', 566),\n",
       " ('ns', 567),\n",
       " ('IN', 568),\n",
       " ('Ġfile', 569),\n",
       " ('ise', 570),\n",
       " ('app', 571),\n",
       " ('mo', 572),\n",
       " ('mat', 573),\n",
       " ('mple', 574),\n",
       " ('Ġvalue', 575),\n",
       " ('data', 576),\n",
       " ('args', 577),\n",
       " ('art', 578),\n",
       " ('ĠD', 579),\n",
       " ('\":', 580),\n",
       " ('Ġel', 581),\n",
       " ('ow', 582),\n",
       " ('ĠG', 583),\n",
       " ('red', 584),\n",
       " ('ly', 585),\n",
       " ('Equal', 586),\n",
       " ('mm', 587),\n",
       " ('ys', 588),\n",
       " ('fig', 589),\n",
       " ('Ġelse', 590),\n",
       " ('value', 591),\n",
       " ('module', 592),\n",
       " ('wor', 593),\n",
       " ('ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 594),\n",
       " ('ĠO', 595),\n",
       " ('Ġar', 596),\n",
       " ('che', 597),\n",
       " ('code', 598),\n",
       " ('est', 599),\n",
       " ('date', 600),\n",
       " ('tri', 601),\n",
       " ('ĠM', 602),\n",
       " ('In', 603),\n",
       " ('thod', 604),\n",
       " ('line', 605),\n",
       " ('AT', 606),\n",
       " ('lic', 607),\n",
       " ('ber', 608),\n",
       " ('Ġma', 609),\n",
       " ('ure', 610),\n",
       " ('sh', 611),\n",
       " ('Ġy', 612),\n",
       " ('01', 613),\n",
       " ('ON', 614),\n",
       " ('quest', 615),\n",
       " ('stance', 616),\n",
       " ('dd', 617),\n",
       " ('ange', 618),\n",
       " ('reate', 619),\n",
       " ('Ġun', 620),\n",
       " ('ource', 621),\n",
       " ('Ġstr', 622),\n",
       " ('\"\"\"', 623),\n",
       " ('Ġthat', 624),\n",
       " ('ang', 625),\n",
       " ('Ġ*', 626),\n",
       " ('assertEqual', 627),\n",
       " ('=\"', 628),\n",
       " ('Ġl', 629),\n",
       " ('Ġpar', 630),\n",
       " ('ist', 631),\n",
       " ('ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 632),\n",
       " ('param', 633),\n",
       " ('ctor', 634),\n",
       " ('Ġ`', 635),\n",
       " ('mode', 636),\n",
       " ('Ġ__', 637),\n",
       " ('val', 638),\n",
       " ('unt', 639),\n",
       " ('url', 640),\n",
       " ('Ġon', 641),\n",
       " ('Ġthis', 642),\n",
       " ('rent', 643),\n",
       " ('Ġit', 644),\n",
       " ('import', 645),\n",
       " ('ac', 646),\n",
       " ('IT', 647),\n",
       " ('Ġ3', 648),\n",
       " ('ry', 649),\n",
       " ('ĠW', 650),\n",
       " ('ES', 651),\n",
       " ('unction', 652),\n",
       " ('py', 653),\n",
       " ('bo', 654),\n",
       " ('ĠN', 655),\n",
       " ('pre', 656),\n",
       " ('Ġen', 657),\n",
       " ('Ġra', 658),\n",
       " ('ild', 659),\n",
       " ('Ġ<', 660),\n",
       " ('Ġset', 661),\n",
       " ('ader', 662),\n",
       " (\"'),\", 663),\n",
       " ('Ġget', 664),\n",
       " ('Ġx', 665),\n",
       " ('ĠE', 666),\n",
       " ('cess', 667),\n",
       " ('ag', 668),\n",
       " ('rint', 669),\n",
       " ('Ġdata', 670),\n",
       " ('Ċĉĉ', 671),\n",
       " ('low', 672),\n",
       " ('Ġuse', 673),\n",
       " (\"Ġ('\", 674),\n",
       " ('sp', 675),\n",
       " ('fa', 676),\n",
       " ('ense', 677),\n",
       " ('ne', 678),\n",
       " ('Ġch', 679),\n",
       " ('om', 680),\n",
       " ('pon', 681),\n",
       " ('AR', 682),\n",
       " ('if', 683),\n",
       " (').', 684),\n",
       " ('Ġfrom', 685),\n",
       " ('roup', 686),\n",
       " ('Ġ>', 687),\n",
       " ('Ġpa', 688),\n",
       " ('ind', 689),\n",
       " ('..', 690),\n",
       " ('ca', 691),\n",
       " ('db', 692),\n",
       " ('atch', 693),\n",
       " ('dir', 694),\n",
       " ('ĠB', 695),\n",
       " ('tp', 696),\n",
       " ('ary', 697),\n",
       " ('oc', 698),\n",
       " ('field', 699),\n",
       " ('ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 700),\n",
       " ('Ġby', 701),\n",
       " ('10', 702),\n",
       " ('Ġwh', 703),\n",
       " ('12', 704),\n",
       " ('32', 705),\n",
       " ('ĠThe', 706),\n",
       " ('Ġassert', 707),\n",
       " ('error', 708),\n",
       " ('ms', 709),\n",
       " ('ip', 710),\n",
       " ('ator', 711),\n",
       " ('cept', 712),\n",
       " ('Con', 713),\n",
       " ('OR', 714),\n",
       " ('ther', 715),\n",
       " ('port', 716),\n",
       " ('ĠTrue', 717),\n",
       " ('ssage', 718),\n",
       " ('__(', 719),\n",
       " ('dex', 720),\n",
       " ('ir', 721),\n",
       " ('vice', 722),\n",
       " ('ore', 723),\n",
       " ('ener', 724),\n",
       " ('ali', 725),\n",
       " ('Ġns', 726),\n",
       " ('ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 727),\n",
       " ('und', 728),\n",
       " ('Ġout', 729),\n",
       " ('ari', 730),\n",
       " ('scri', 731),\n",
       " ('Ġtype', 732),\n",
       " ('ten', 733),\n",
       " ('tes', 734),\n",
       " ('ĠU', 735),\n",
       " ('ponse', 736),\n",
       " ('Ġat', 737),\n",
       " ('ok', 738),\n",
       " ('jo', 739),\n",
       " ('os', 740),\n",
       " ('Ġos', 741),\n",
       " ('read', 742),\n",
       " ('append', 743),\n",
       " ('Ġraise', 744),\n",
       " ('EN', 745),\n",
       " ('AL', 746),\n",
       " ('pec', 747),\n",
       " ('rite', 748),\n",
       " ('user', 749),\n",
       " ('late', 750),\n",
       " ('################', 751),\n",
       " ('ĠĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 752),\n",
       " ('ml', 753),\n",
       " (\"Ġ['\", 754),\n",
       " ('ress', 755),\n",
       " ('tic', 756),\n",
       " ('Ġresult', 757),\n",
       " ('ument', 758),\n",
       " ('ĠFalse', 759),\n",
       " ('64', 760),\n",
       " (\"'):\", 761),\n",
       " ('col', 762),\n",
       " ('uth', 763),\n",
       " ('----------------', 764),\n",
       " ('wa', 765),\n",
       " ('ĠĠĠĠĠĠ', 766),\n",
       " ('fe', 767),\n",
       " ('========', 768),\n",
       " ('Ġ@', 769),\n",
       " ('Ġlist', 770),\n",
       " ('ock', 771),\n",
       " ('lib', 772),\n",
       " ('Ġal', 773),\n",
       " ('ded', 774),\n",
       " ('gn', 775),\n",
       " ('ET', 776),\n",
       " ('Ġare', 777),\n",
       " ('tent', 778),\n",
       " ('Ġ)', 779),\n",
       " ('Ġwe', 780),\n",
       " ('Ġkey', 781),\n",
       " ('object', 782),\n",
       " ('ill', 783),\n",
       " ('Ġ}', 784),\n",
       " ('Test', 785),\n",
       " ('ds', 786),\n",
       " ('Field', 787),\n",
       " ('ings', 788),\n",
       " ('([', 789),\n",
       " ('qui', 790),\n",
       " ('method', 791),\n",
       " ('ĠV', 792),\n",
       " ('temp', 793),\n",
       " ('log', 794),\n",
       " ('dict', 795),\n",
       " ('ĠRe', 796),\n",
       " ('vent', 797),\n",
       " ('icense', 798),\n",
       " ('pp', 799),\n",
       " ('lay', 800),\n",
       " ('ray', 801),\n",
       " ('////', 802),\n",
       " ('ans', 803),\n",
       " ('tribu', 804),\n",
       " ('25', 805),\n",
       " (\"Ġ{'\", 806),\n",
       " ('ĊĊĠ', 807),\n",
       " ('ave', 808),\n",
       " ('Type', 809),\n",
       " ('Ġver', 810),\n",
       " ('kw', 811),\n",
       " ('ception', 812),\n",
       " ('ask', 813),\n",
       " ('False', 814),\n",
       " ('heck', 815),\n",
       " ('info', 816),\n",
       " ('ou', 817),\n",
       " ('stri', 818),\n",
       " ('work', 819),\n",
       " ('Ġcl', 820),\n",
       " ('Ġfunction', 821),\n",
       " ('ces', 822),\n",
       " ('ren', 823),\n",
       " ('Ġlen', 824),\n",
       " ('ĠR', 825),\n",
       " ('ble', 826),\n",
       " ('Ġmode', 827),\n",
       " ('min', 828),\n",
       " ('ph', 829),\n",
       " ('11', 830),\n",
       " ('arget', 831),\n",
       " ('ST', 832),\n",
       " ('ec', 833),\n",
       " ('count', 834),\n",
       " ('():', 835),\n",
       " ('init', 836),\n",
       " ('Ġ4', 837),\n",
       " ('son', 838),\n",
       " ('ust', 839),\n",
       " ('ger', 840),\n",
       " ('den', 841),\n",
       " ('ext', 842),\n",
       " ('ĠLicense', 843),\n",
       " ('Ġdefault', 844),\n",
       " ('.__', 845),\n",
       " ('mmand', 846),\n",
       " ('uld', 847),\n",
       " ('Ġ+=', 848),\n",
       " ('tions', 849),\n",
       " ('lient', 850),\n",
       " ('net', 851),\n",
       " ('tive', 852),\n",
       " ('lock', 853),\n",
       " ('jang', 854),\n",
       " ('ant', 855),\n",
       " ('ree', 856),\n",
       " ('ample', 857),\n",
       " ('his', 858),\n",
       " ('ft', 859),\n",
       " ('jango', 860),\n",
       " ('instance', 861),\n",
       " ('lif', 862),\n",
       " ('Ġmodule', 863),\n",
       " ('Ġtry', 864),\n",
       " ('comp', 865),\n",
       " ('Ġexcept', 866),\n",
       " ('andle', 867),\n",
       " ('Ġprint', 868),\n",
       " ('ĊĠĠĠĠĠĠĠĠĠ', 869),\n",
       " ('ĠH', 870),\n",
       " ('Ġpre', 871),\n",
       " ('tain', 872),\n",
       " ('size', 873),\n",
       " ('Ġcls', 874),\n",
       " ('ses', 875),\n",
       " ('try', 876),\n",
       " ('04', 877),\n",
       " ('config', 878),\n",
       " ('De', 879),\n",
       " ('Ġso', 880),\n",
       " ('Ġcan', 881),\n",
       " ('no', 882),\n",
       " ('Ġdo', 883),\n",
       " ('pen', 884),\n",
       " ('AN', 885),\n",
       " ('default', 886),\n",
       " ('gin', 887),\n",
       " ('Ġfield', 888),\n",
       " ('mb', 889),\n",
       " ('cur', 890),\n",
       " ('back', 891),\n",
       " ('string', 892),\n",
       " ('ters', 893),\n",
       " ('Ġtime', 894),\n",
       " ('thon', 895),\n",
       " ('Ġnew', 896),\n",
       " ('SE', 897),\n",
       " ('umber', 898),\n",
       " ('Ġro', 899),\n",
       " (\"''\", 900),\n",
       " ('[\"', 901),\n",
       " ('join', 902),\n",
       " ('atus', 903),\n",
       " ('ape', 904),\n",
       " ('RE', 905),\n",
       " ('LE', 906),\n",
       " ('abel', 907),\n",
       " ('Ġobject', 908),\n",
       " ('Ġwill', 909),\n",
       " ('peci', 910),\n",
       " ('uc', 911),\n",
       " ('Ġuser', 912),\n",
       " ('format', 913),\n",
       " ('ll', 914),\n",
       " ('gra', 915),\n",
       " (\"'}\", 916),\n",
       " ('load', 917),\n",
       " ('ong', 918),\n",
       " ('Ex', 919),\n",
       " ('Ġsi', 920),\n",
       " ('Ġsho', 921),\n",
       " ('Pro', 922),\n",
       " ('ght', 923),\n",
       " ('Ġelif', 924),\n",
       " ('group', 925),\n",
       " ('CT', 926),\n",
       " ('Ġpath', 927),\n",
       " ('uct', 928),\n",
       " ('start', 929),\n",
       " ('node', 930),\n",
       " ('UT', 931),\n",
       " ('opy', 932),\n",
       " ('sub', 933),\n",
       " ('kwargs', 934),\n",
       " ('\"]', 935),\n",
       " ('cal', 936),\n",
       " ('St', 937),\n",
       " ('://', 938),\n",
       " ('own', 939),\n",
       " ('ute', 940),\n",
       " ('state', 941),\n",
       " ('sed', 942),\n",
       " ('write', 943),\n",
       " ('uti', 944),\n",
       " ('Ġhe', 945),\n",
       " ('ume', 946),\n",
       " ('Ġ[]', 947),\n",
       " ('Ġno', 948),\n",
       " ('Ġoption', 949),\n",
       " ('ken', 950),\n",
       " ('ĠThis', 951),\n",
       " ('Ġlog', 952),\n",
       " ('ject', 953),\n",
       " ('mage', 954),\n",
       " ('Ġ5', 955),\n",
       " ('ost', 956),\n",
       " ('fields', 957),\n",
       " ('Ġ\\\\', 958),\n",
       " ('form', 959),\n",
       " ('Ġhas', 960),\n",
       " ('16', 961),\n",
       " ('scription', 962),\n",
       " ('attr', 963),\n",
       " ('13', 964),\n",
       " ('ĊĊĠĠĠĠĠĠĠĠĠĠĠ', 965),\n",
       " ('ariable', 966),\n",
       " ('ĠIf', 967),\n",
       " ('Ġ##', 968),\n",
       " ('ull', 969),\n",
       " ('Co', 970),\n",
       " ('da', 971),\n",
       " ('lement', 972),\n",
       " ('Ġnp', 973),\n",
       " ('ated', 974),\n",
       " ('ID', 975),\n",
       " ('``', 976),\n",
       " ('cor', 977),\n",
       " ('02', 978),\n",
       " ('nd', 979),\n",
       " ('Ġpass', 980),\n",
       " ('create', 981),\n",
       " ('Ġconst', 982),\n",
       " ('com', 983),\n",
       " ('main', 984),\n",
       " ('Ġ**', 985),\n",
       " ('json', 986),\n",
       " ('99', 987),\n",
       " ('IC', 988),\n",
       " ('Ġsys', 989),\n",
       " ('fix', 990),\n",
       " ('ery', 991),\n",
       " ('Ġsub', 992),\n",
       " ('og', 993),\n",
       " ('by', 994),\n",
       " ('0000', 995),\n",
       " ('Ġall', 996),\n",
       " (\"'\\\\\", 997),\n",
       " ('15', 998),\n",
       " ('gth', 999),\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a82fbcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'  '\n",
      "'    '\n",
      "'   '\n",
      "'        '\n",
      "'se'\n",
      "'in'\n",
      "'       '\n",
      "'re'\n",
      "'on'\n",
      "'te'\n",
      "'\\n       '\n",
      "'\\n        '\n",
      "'or'\n",
      "'st'\n",
      "'de'\n",
      "'\\n   '\n",
      "'th'\n",
      "'le'\n",
      "' ='\n",
      "'lf'\n",
      "'self'\n",
      "'me'\n",
      "'al'\n"
     ]
    }
   ],
   "source": [
    "for x in [t for t, _ in tokens[257:280]]:\n",
    "    temp = new_tokenizer.convert_tokens_to_string(list(x))\n",
    "    print(repr(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201f113",
   "metadata": {},
   "source": [
    "#### 마지막 단어를 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c533b64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' capt'\n",
      "' embedded'\n",
      "' regarding'\n",
      "'Bundle'\n",
      "'355'\n",
      "' recv'\n",
      "' dmp'\n",
      "' vault'\n",
      "' Mongo'\n",
      "' possibly'\n",
      "'implementation'\n",
      "'Matches'\n"
     ]
    }
   ],
   "source": [
    "for x in [t for t, _ in tokens[-12:]]:\n",
    "    temp = new_tokenizer.convert_tokens_to_string(list(x))\n",
    "    print(repr(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc556da",
   "metadata": {},
   "source": [
    "토크나이저가 어떻게 동작하는지 보기 위해 간단한 파이썬 예제 코드를 토큰화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49fbf072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġs', 'ay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWor', 'ld', '!\")', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 's', 'ay', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d2e65",
   "metadata": {},
   "source": [
    "#### 프로그램 키워드는 아니지만 토크나이저가 World나 say 같은 평범한 영단어를 분할한다.  \n",
    "-> 이런 단어가 말뭉치에 자주 등장하리라 예상되는데 마음에 좀 걸린다.  \n",
    "-> 파이썬의 예약어가 모두 어휘사전에 있는지 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5509c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬 전체 예약어 개수: 35\n",
      "예약어 'await'는 어휘 사전에 없습니다.\n",
      "예약어 'finally'는 어휘 사전에 없습니다.\n",
      "예약어 'nonlocal'는 어휘 사전에 없습니다.\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "\n",
    "print(f'파이썬 전체 예약어 개수: {len(keyword.kwlist)}')\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw in keyword.kwlist:\n",
    "        if keyw not in new_tokenizer.vocab:\n",
    "            print(f\"예약어 '{keyw}'는 어휘 사전에 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a0829",
   "metadata": {},
   "source": [
    "#### finally 같이 매우 자주 등장하는 예약어가 없다.  \n",
    "#### 따라서 데이터셋에서 더 많은 샘플을 가져와 더 큰 어휘사전을 만들겠다.  \n",
    "#### 어휘사전을 32,768개 단어로 구성하고(8의 배수가 GPU/TPU 계산에 더 효율적이다.)  \n",
    "#### 토크나이저를 더 많은 말뭉치 데이터에서 훈련하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "809e80f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea1b055f5aa4439bf224c40fbe5ad52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = 200000\n",
    "new_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(),\n",
    "                                                         vocab_size=32768,\n",
    "                                                         initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebdf829",
   "metadata": {},
   "source": [
    "더 많은 문서를 추가해도 가장 자주 등장한 토큰이 크게 바뀌리라고 기대하진 않는다.  \n",
    "하지만 마지막 토큰을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24c5275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(new_tokenizer_larger.vocab.items(), \n",
    "                key=lambda x: x[1],\n",
    "                reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f691bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" '<?\"\n",
      "'Functional'\n",
      "' Images'\n",
      "'encoders'\n",
      "' bibrec'\n",
      "' OPTIONAL'\n",
      "' rdclass'\n",
      "'SocketAddressTag'\n",
      "'资金'\n",
      "'DEPLOYMENT'\n",
      "'经纪公司代码'\n",
      "\")'],\"\n"
     ]
    }
   ],
   "source": [
    "for x in [t for t, _ in tokens[-12:]]:\n",
    "    temp = new_tokenizer_larger.convert_tokens_to_string(list(x))\n",
    "    print(repr(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e049c",
   "metadata": {},
   "source": [
    "#### 새 토크나이저로 샘플 코드를 토큰화하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c32580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\")', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer_larger(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84176b25",
   "metadata": {},
   "source": [
    "#### 여기서도 편리하게 들여쓰기가 어휘사전에 유지됐으며 Hello, World, say 같은 평범한 영단어도 하나의 토큰으로 포함된다.  \n",
    "모델이 후속 작업에서 데이터를 처리할 때 기대할 수 있는 결과와 잘 맞는 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14db7e5",
   "metadata": {},
   "source": [
    "앞에서와 같이 어휘사전에 없는 파이썬 예약어를 조사해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06feb69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예약어 'nonlocal'는 어휘사전에 없습니다.\n"
     ]
    }
   ],
   "source": [
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer_larger.vocab:\n",
    "        print(f\"예약어 '{keyw}'는 어휘사전에 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1f79c",
   "metadata": {},
   "source": [
    "여전히 어휘사전에 nonlocal 예약어가 없는데, 이 단어는 구문을 복잡하게 만들어서 실제로도 드물게 사용된다.  \n",
    "따라서 어휘사전에 포함시키지 않는 것이 합당하다(고 책에서는 말함)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7f8ff",
   "metadata": {},
   "source": [
    "수동으로 조사해보니 새로운 토크나이저가 이 작업에 잘 맞을 것 같다고 한다.  \n",
    "하지만 앞서 언급한 대로 모델의 성능을 측정하지 않고 토크나이저의 성능을 객관적으로 평가하기는 어렵다.  \n",
    "이 토크나이저를 사용해 모델을 훈련한 후에 실제로 얼마나 잘 동작하는지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098ca90",
   "metadata": {},
   "source": [
    "※ 토큰화된 코드 샘플의 시퀀스 길이를 비교해서 새 토크나이저가 기본 GPT-2 토크나이저보다 거의 두 배 더 효율적임을 쉽게 확인했다.  \n",
    "  \n",
    "새 토크나이저는 기존 토크나이저가 텍스트를 인코딩할 때 사용한 토큰의 약 절반만 사용했다.  \n",
    "이로 인해 아무런 비용을 들이지 않고 실제적인 모델의 문맥 크기가 두 배로 늘어난다.  \n",
    "문맥 윈도 크기 1,024에서 새로운 토크나이저로 모델을 훈련하는 것은 문맥 윈도 크기 2,048에서 기존 토크나이저로 모델을 훈련하는 것과 동일하다.  \n",
    "  \n",
    "하지만 훨씬 더 빠르고 메모리 효율성도 더 높다는 이점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a4653b",
   "metadata": {},
   "source": [
    "## 10.2.5 허브에 사용자 정의 토크나이저 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d3053",
   "metadata": {},
   "source": [
    "토크나이저를 훈련했으니 저장해야 한다.  \n",
    "토크나이저를 저장하고 나중에 어디에서든 가져다 쓸 수 있게 하려면 허깅페이스 허브에 업로드해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7a69ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# model = AutoModel.from_pretrained('업로드할_모델_경로')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('업로드할_모델_경로')\n",
    "\n",
    "# # Huggingface Access Token\n",
    "# ACCESS_TOKEN = '액세스_토큰_붙여넣기'\n",
    "\n",
    "# # Upload to Huggingface\n",
    "# model.push_to_hub('Huggingface_Repo_이름', use_temp_dir=True, use_auth_token=ACCESS_TOKEN)\n",
    "# tokenizer.push_to_hub('Huggingface_Repo_이름', use_temp_dir=True, use_auth_token=ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b89050c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jj/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/utils/hub.py:652: UserWarning: The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/bh8648/codeparrot/commit/6fdacbe89b012a13e4deb6b3bcfa41699a104f54', commit_message='Upload tokenizer', commit_description='', oid='6fdacbe89b012a13e4deb6b3bcfa41699a104f54', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 터미널에서 허깅페이스에 로그인 한 뒤\n",
    "\n",
    "model_ckpt = \"codeparrot\"\n",
    "org = \"bh8648\"\n",
    "new_tokenizer_larger.push_to_hub(model_ckpt, organization=org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d24d66",
   "metadata": {},
   "source": [
    "어떤 조직 내에 푸시하고 싶지 않다면 organization 매개변수를 빼면 된다.  \n",
    "그러면 자신의 네임스페이스 안에 codeparrot이란 이름의 저장소가 만들어지고 누구든지 다음 코드를 실행해 이 토크나이저를 로드할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3146eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\")', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "# reloaded_tokenizer = AutoTokenizer.from_pretrained('bh8648/codeparrot')\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(org + '/' + model_ckpt)\n",
    "print(reloaded_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2910e60",
   "metadata": {},
   "source": [
    "#### 허브에 저장된 어휘사전과 파일을 조사할 수도 있다.\n",
    "나중에 재현할 수 있도록 새 토크나이저를 조금만 재훈련시켰던 new_tokenizer도 저장해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce1752c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/bh8648/codeparrot-small-vocabulary/commit/414f02f6e4632a6c5629542387fc8f3e7de5a4a3', commit_message='Upload tokenizer', commit_description='', oid='414f02f6e4632a6c5629542387fc8f3e7de5a4a3', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.push_to_hub(model_ckpt + \"-small-vocabulary\", organization=org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91cd922",
   "metadata": {},
   "source": [
    "# 10.3 밑바닥부터 모델을 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7c700",
   "metadata": {},
   "source": [
    "### 코잘 언어 모델링 causal language modeling\n",
    "-> 우리가 하려는 작업은 코드 샘플 시작 부분을 모델에게 제공하고 코드의 나머지 부분을 생성해 완성하라고 요청하는 것이다.  \n",
    "\n",
    "-> 레이블이 없는 데이터셋을 사용하는 자기 지도 훈련 목표. = 코잘 언어 모델링  \n",
    "  \n",
    "-> 코드 자동 완성은 이와 직접적으로 관련된 후속 작업임.(이런 작업에는 GPT 계열 같은 디코더 전용 아키텍처가 가장 잘 맞다.)\n",
    "#### ※ 코잘 언어 모델링에서 모델은 마스킹된 미래 토큰을 예측한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e618cb",
   "metadata": {},
   "source": [
    "### 마스크드 언어 모델링\n",
    "-> 관련성이 있지만 조금 다른 작업은 모델에게 잡음이 섞인 코드 샘플(랜덤한 단어나 마스킹된 단어로 코드 명령을 바꾸는 등)을 주고 깨끗한 원본 샘플을 재구성하라고 요청하는 것\n",
    "  \n",
    "-> 이것도 자기 지도 훈련 목표이며, 일반적으로 마스크드 언어 모델링masked language modeling 또는 잡음 제거 목표denoising objective라 한다.  \n",
    "  \n",
    "-> 많은 모델이 이런 식으로 사전 훈련되며, 레이블링된 샘플이 제한적인 후속 작업의 경우 대규모 말뭉치에서 훈련된 마스크드 언어 모델을 사용해 모델을 미세 튜닝할 수 있다.\n",
    "#### ※ 마스크드 언어 모델링에서는 입력 토큰 중 일부가 마스킹되거나 바뀌며 이후 원본 토큰을 예측하는 것이 모델의 작업이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe8186",
   "metadata": {},
   "source": [
    "### 시퀀스-투-시퀀스 훈련\n",
    "-> 정규식 같은 수동 규칙으로 주석이나 독스트링을 코드에서 분리해서 레이블링된 데이터셋으로 사용하도록 (코드, 주석) 쌍의 대규모 데이터셋을 구축하는 작업.  \n",
    "  \n",
    "-> 훈련은 한 카테고리(코드나 주석)를 모델의 입력으로 사용하고 다른 카테고리(주석이나 코드)를 레이블로 사용하는 지도 학습 목표가 된다.  \n",
    "  \n",
    "-> 즉, (입력, 레이블) 쌍을 사용하는 지도 학습 문제가 된다.\n",
    "\n",
    "-> 이런 지도 학습 훈련 작업에 직접적으로 관련된 후속 작업은 입력/출력을 어떻게 지정하느냐에 따라 코드로부터 문서를 생성하거나 문서로부터 코드를 생성하는 일이다.  \n",
    "  \n",
    "-> 이런 설정에서 한 시퀀스는 다른 시퀀스로 변환되며 T5, BART, PEGASUS 같은 인코더-디코더 모델이 여기에 잘맞다.\n",
    "#### ※ 시퀀스-투-시퀀스 작업을 위한 인코더-디코더 아키텍처에서 입력은 경험적인 규칙을 사용해 주석/코드 쌍으로 분할되고, 모델은 주석과 코드 중 하나를 입력으로 받아 다른 하나를 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544807f",
   "metadata": {},
   "source": [
    "### 코드 자동 완성 모델을 구축하고 싶기 때문에 첫 번째 훈련 목표와 GPT 아키텍처를 선택한다. 이제 새로운 GPT-2 모델을 생성해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8044310",
   "metadata": {},
   "source": [
    "## 10.3.2 모델 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4b420",
   "metadata": {},
   "source": [
    "from_pretrained() 메서드로 모델을 로드하지 않고 새 모델을 만든다.  \n",
    "  \n",
    "하지만 gpt2-xl 설정과 동일한 하이퍼파라미터를 사용하고 새 토크나이저를 위해 어휘사전의 크기만 바꾼다.  \n",
    "\n",
    "그다음 이 설정을 from_config() 메서드에 사용해 새 모델을 초기화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46c87d81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "org = 'bh8648'\n",
    "model_ckpt = \"codeparrot\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(org + '/' + model_ckpt)\n",
    "config = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e793ec0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86247185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='bh8648/codeparrot', vocab_size=32768, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d4b6404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(32768, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241249b",
   "metadata": {},
   "source": [
    "wte: word token embedding  \n",
    "wpe: word position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c247a",
   "metadata": {},
   "source": [
    "### 모델이 얼마나 큰지 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62066ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 (xl) 크기: 1529.6M parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT-2 (xl) 크기: {model_size(model) / 1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d898b3c8",
   "metadata": {},
   "source": [
    "15억 개 파라미터가 있다. 용량이 크지만 준비한 데이터셋도 대용량이다.  \n",
    "보통 대규모 언어 모델은 데이터셋이 충분히 크기만 하면 효율적인 훈련이 가능하다.  \n",
    "새로 초기화한 모델을 models/ 폴더에 저장하고 허브에 푸시해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36dc289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True,\n",
    "                      organization=org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0a673",
   "metadata": {},
   "source": [
    "체크포인트 크기가 5GB보다 크기 때문에 모델을 허브에 푸시하는 데 몇 분이 걸릴 수 있다.  \n",
    "모델이 매우 크기 때문에 모든 것이 잘 동작하는지 확인하기 위해 작은 버전도 만들겠다.  \n",
    "표준 GPT-2 크기를 기본으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73ffa9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codeparrot'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ad36006",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 크기: 111.0M개의 파라미터\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(org + '/' + model_ckpt)\n",
    "config_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\n",
    "model_small = AutoModelForCausalLM.from_config(config_small)\n",
    "print(f\"GPT-2 크기: {model_size(model_small)/1000**2:.1f}M개의 파라미터\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a61d06",
   "metadata": {},
   "source": [
    "#### 이 모델도 쉽게 공유하고 재사용하기 위해 허브에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7471622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small.save_pretrained(\"models/\" + model_ckpt + \"-small\", push_to_hub=True,\n",
    "                             organization=org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26522cfe",
   "metadata": {},
   "source": [
    "두 개의 모델을 준비했으니, 이제 훈련할 때 효율적으로 입력 데이터를 주입할 방법이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26051721",
   "metadata": {},
   "source": [
    "## 10.3.3 데이터로더 구축하기\n",
    "훈련 효율을 극대화하도록 문맥 크기를 꽉 채운 시퀀스를 모델에 제공한다.  \n",
    "  \n",
    "예를 들어 모델의 문맥 크기가 1,024개 토큰이라면 훈련할 때 매번 1,024개 토큰의 시퀀스를 제공하는 것이 좋다.  \n",
    "하지만 일부 코드 샘플은 토큰의 개수가 1,024개보다 더 짧거나 길다.  \n",
    "\n",
    "이 경우 모델에게 sequence_length 길이 시퀀스의 배치를 주입하기 위해 시퀀스 마지막을 자르거나 패딩해야 한다.    \n",
    "하지만 그러면 훈련의 효율성이 조금 떨어지고 패딩된 토큰의 '레이블'을 '패딩'하고 '마스킹'해야 한다.  \n",
    "-> 데이터 제약에 비해 컴퓨팅 제약이 훨씬 더 커진다.  \n",
    "  \n",
    "따라서 여기서는 시퀀스 마지막 부분을 너무 많이 잃지 않도록 하는 쉽고 효율적인 방법을 사용하려 한다.  \n",
    "1. 여러 샘플을 토큰화한 다음 EOS 토큰으로 이를 연결해 매우 긴 시퀀스를 만든다.  \n",
    "2. 마지막으로 이 시퀀스를 동일한 크기의 청크chunk로 나눈다.  \n",
    "\n",
    "-> 이 방법을 사용하면 마지막 데이터에서 손실되는 부분이 미미하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e9cc9",
   "metadata": {},
   "source": [
    "#### 예를 들어 입력 문자열의 문자 개수를 다음과 같이 정의해서 토큰화된 샘플에 약 100개의 완전한 시퀀스가 있게 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0de7852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sequences = 100    # 토크나이저로부터 얻으려는 (잘린) 시퀀스의 개수(가령 100)\n",
    "sequence_length = 1024       # 토크나이저가 반환한 각 시퀀스의 토큰 개수 (가령 1024)\n",
    "characters_per_token = 500  # 사전에 추정해야 하는 각 출력 토큰의 평균 문자 개수\n",
    "\n",
    "input_characters = number_of_sequences * sequence_length * characters_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0d5e2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51200000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_characters # 토크나이저에 입력된 문자열에 있는 문자의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2202bba",
   "metadata": {},
   "source": [
    "input_characters 개의 문자로 된 문자열을 입력하면 평균적으로 number_of_sequences 개의 출력 시퀀스를 얻는다.  \n",
    "number_of_sequence = 100은 시퀀스를 약 100개 쌓고 기껏해야 너무 짧거나 긴 마지막 원소를 잃는다는 의미다. 즉, 데이터셋의 1%를 잃는 셈.  \n",
    "동시에 이 방식은 대부분 파일의 끝을 잘라내지 않아 편향을 일으키지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d72c5",
   "metadata": {},
   "source": [
    "#### 먼저 데이터셋에 있는 토큰의 평균 문자 길이를 예측해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ead87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772948ed0e3b4a8d86b46ce583ae9414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples, total_characters, total_tokens = 500, 0, 0\n",
    "dataset = load_dataset(\"transformersbook/codeparrot-train\", split=\"train\",\n",
    "                       streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e65696f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2f4cdc7b6f4498bd873433a7ab0438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2599 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6231516195736053\n"
     ]
    }
   ],
   "source": [
    "for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
    "    total_characters += len(example[\"content\"])\n",
    "    total_tokens += len(tokenizer(example[\"content\"]).tokens())\n",
    "\n",
    "characters_per_token = total_characters / total_tokens\n",
    "\n",
    "print(characters_per_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c34e6e",
   "metadata": {},
   "source": [
    "#### 평균적으로 각 토큰당 약 3.6의 character를 가진다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef3b91",
   "metadata": {},
   "source": [
    "모델에게 일정한 길이의 입력을 주입하도록 사용자 정의 IterableDataset (파이토치에서 제공하는 헬퍼 클래스)를 만들기 위해 필요한 것이 모두 준비됐다.  \n",
    "  \n",
    "#### IterableDataset을 상속해서 방금 살펴본 로직을 기반으로 다음 원소를 반환하는 \\__iter__() 함수를 작성해보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fa61eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, dataset, seq_length=1024, num_of_sequences=1024, \n",
    "                 chars_per_token=3.6):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
    "    \n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    m=f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    break\n",
    "                try:\n",
    "                    m=f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    buffer.append(next(iterator)[\"content\"])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    iterator = iter(self.dataset)\n",
    "            \n",
    "            all_token_ids = []\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False) # truncation은 문장 잘림을 허용할지 말지 정하는 것\n",
    "            for tokenized_input in tokenized_inputs['input_ids']:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id]) # 여기가 모든 시퀀스를 토큰화 한 다음 각 시퀀스의 끝에 EOS 토큰만 붙이고 하나로 쭉 연결하는 작업\n",
    "            \n",
    "            for i in range(0, len(all_token_ids), self.seq_length): # 여기가 모두 이어 붙인 시퀀스(토큰화되어있음)를 chunk에 맞춰 구분짓는 곳\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188acdc9",
   "metadata": {},
   "source": [
    "\\__iter__() 메서드는 충분한 개수의 문자를 포함할 때까지 문자열 버퍼를 채운다.  \n",
    "  \n",
    "버퍼 안의 모든 원소는 토큰화되어 EOS 토큰으로 연결되고 그다음 all_token_ids에 있는 긴 시퀀스가 seq_length 크기로 나뉜다.  \n",
    "  \n",
    "일반적으로 패딩된 가변 길이 시퀀스를 쌓을 때는 훈련 시 패딩을 무시하도록 어텐션 마스크가 필요하다.  \n",
    "여기서는 동일한 (최대) 길이 시퀀스만 제공하므로 마스크가 필요하지 않으며 input_ids만 반환하면 된다.  \n",
    "  \n",
    "이 사용자 정의 데이터셋을 테스트해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f4a2d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill buffer: 0<36864\n",
      "Fill buffer: 3806<36864\n",
      "Fill buffer: 5182<36864\n",
      "Fill buffer: 6297<36864\n",
      "Fill buffer: 7366<36864\n",
      "Fill buffer: 11481<36864\n",
      "Fill buffer: 17562<36864\n",
      "Fill buffer: 22827<36864\n",
      "Fill buffer: 29806<36864\n",
      "Buffer full: 44530>=36864\n",
      "시퀀스 길이: [1024, 1024, 1024, 1024, 1024]\n"
     ]
    }
   ],
   "source": [
    "shuffled_dataset = dataset.shuffle(buffer_size=100) # 입력된 buffer_size 만큼 data를 채우고(처음부터 순서대로) 무작위로 섞음\n",
    "constant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset,\n",
    "                                                num_of_sequences=10)\n",
    "dataset_iterator = iter(constant_length_dataset)\n",
    "\n",
    "lengths = [len(b) for _, b in zip(range(5), dataset_iterator)] # 5개만 시퀀스 길이 꺼내보기\n",
    "print(f\"시퀀스 길이: {lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa3799",
   "metadata": {},
   "source": [
    "의도대로 작동해 모델에 전달할 일정 길의의 입력을 얻었다.  \n",
    "이제 모델을 위해 신뢰할만한 데이터 소스를 갖췄으니 실제 훈련 루프를 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5e835",
   "metadata": {},
   "source": [
    "#### ※ ConstantLengthDataset을 만들기 전에 원시 데이터셋을 섞었다.  \n",
    "IterableDataset 이므로 처음에 전체 데이터셋을 섞을 수 없다.  \n",
    "대신 데이터셋에서 원소를 가져오기 전에 buffer_size 크기의 버퍼를 할당하고 버퍼 안의 원소를 섞는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709d536",
   "metadata": {},
   "source": [
    "## 10.3.4 훈련 루프 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164d7ba",
   "metadata": {},
   "source": [
    "사용자 정의 언어 모델을 훈련할 때 분명한 제약 조건은 GPU의 메모리 제한이다.  \n",
    "이 예제에서는 여러 개의 GPU를 훈련에 사용하기 위해 '데이터 병렬화(data parallelism)'을 구현한다.  \n",
    "  \n",
    "다행히 액셀러레이트를 사용해 예제 코드를 확장할 수 있다.  \n",
    "액셀레레이트 라이브러리는 분산훈련을 위해 하드웨어를 쉽게 바꿀 수 있게 고안됐다.  \n",
    "분산 훈련을 위해 Trainer를 사용할 수도 있지만, 액셀러레이트를 사용하면 훈련 루프를 완전하게 제어할 수 있으니 여기서 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e5a26",
   "metadata": {},
   "source": [
    "액셀러레이트는 혼합 정밀도와 어떤 종류의 분산 환경(단일 GPU, 다중 GPU, TPU)에서도 훈련 스크립트를 실행할 수 있는 간편한 API를 제공한다.  \n",
    "동일한 코드를 디버깅하기 위해 로컬 컴퓨터에서 실행하거나 최종 훈련을 위해 대규모 훈련 클러스터에서도 실행이 가능하다.  \n",
    "  \n",
    "기본 파이토치 훈련 루프에서 약간만 변경하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc1673cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55e7ff",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model = torch.nn.Transformer()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "dataset = load_dataset('my_dataset')\n",
    "data = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
    "\n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for source, targets in data:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(source)\n",
    "        loss = F.cross_entropy(output, targets)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1449f5",
   "metadata": {},
   "source": [
    "#### 바뀐 부분 중 핵심은 prepare() 메서드 호출이다.\n",
    "모델, 옵티마이저, 데이터로더를 모두 준비하고 인프라에 분산한다.  \n",
    "이렇게 파이토치 훈련루프를 조금 바꿔 다양한 인프라로 훈련을 확장한다.  \n",
    "  \n",
    "이 점을 유념하면서 훈련 스크립트를 만들고 몇 개의 헬퍼 함수를 정의하겠다.\n",
    "먼저 훈련을 위한 하이퍼파라미터를 설정하고 접근이 용이하도록 Namespace로 감싼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d543c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# 작은 모델에 해당하는 파라미터\n",
    "config = {\"train_batch_size\": 2, # 12\n",
    "          \"valid_batch_size\": 2, # 12\n",
    "          \"weight_decay\": 0.1,\n",
    "          \"shuffle_buffer\": 1000,\n",
    "          \"learning_rate\": 2e-4, # 5e-4\n",
    "          \"lr_scheduler_type\": \"cosine\",\n",
    "          \"num_warmup_steps\": 750, # 2000\n",
    "          \"gradient_accumulation_steps\": 16, # 1\n",
    "          \"max_train_steps\": 50000, # 150000\n",
    "          \"max_eval_steps\": -1,\n",
    "          \"seq_length\": 1024,\n",
    "          \"seed\": 1,\n",
    "          \"save_checkpoint_steps\": 50000} # 15000\n",
    "\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "640ae9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(gradient_accumulation_steps=16, learning_rate=0.0002, lr_scheduler_type='cosine', max_eval_steps=-1, max_train_steps=50000, num_warmup_steps=750, save_checkpoint_steps=50000, seed=1, seq_length=1024, shuffle_buffer=1000, train_batch_size=2, valid_batch_size=2, weight_decay=0.1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495711a0",
   "metadata": {},
   "source": [
    "### 그다음 훈련을 위핸 로깅을 설정한다.  \n",
    "모델을 밑바닥부터 훈련하기 떄문에 훈련 시간이 조금 걸리고 고가의 인프라가 필요하다.  \n",
    "따라서 관련된 모든 정보를 저장하고 쉽게 참조할 수 있게 만드는 것이 좋다.  \n",
    "  \n",
    "setup_logging() 메서드는 세 개의 로깅 수준을 설정한다.  \n",
    "표준 파이썬 Logger(https://oreil.ly/P9Xrm), 텐서보드 (https://oreil.ly/kY5ri),  \n",
    "wandb(https://oreil.ly/BCC3k) 를 사용한다.  \n",
    "주어진 문제와 선호하는 바에 따라 로깅 프레임워크를 추가하거나 삭제할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e16c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "import wandb\n",
    "\n",
    "def setup_logging(project_name):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, handlers=[\n",
    "            logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n",
    "            logging.StreamHandler()])\n",
    "    if accelerator.is_main_process: # 로깅을 한 번만 설정한다.\n",
    "        wandb.init(project=project_name, config=args)\n",
    "        run_name = wandb.run.name\n",
    "        tb_writer = SummaryWriter()\n",
    "        tb_writer.add_hparams(vars(args), {'0':0})\n",
    "        logger.setLevel(logging.INFO)\n",
    "        datasets.utils.logging.set_verbosity_debug()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        tb_writer = None\n",
    "        run_name = ''\n",
    "        logger.setLevel(logging.ERROR)\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    \n",
    "    return logger, tb_writer, run_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fea8eb",
   "metadata": {},
   "source": [
    "각 워커(worker)는 고유한 accelerator.process_index를 받는다.  \n",
    "이를 FileHandler에 사용해 각 워커의 로그를 개별 파일에 기록한다.  \n",
    "  \n",
    "또 메인 워커에서만 true인 accelerator.is_main_process 속성도 사용한다.  \n",
    "이를 사용해 텐서보드와 wandb의 로거logger가 여러 번 초기화되지 않게 하고 다른 워커에서 로깅 수준을 낮춘다.  \n",
    "나중에 허브에서 실험 결과에 이름을 부여하기 위해 자동으로 생성된 고유한 wandb.run.name을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75704199",
   "metadata": {},
   "source": [
    "#### 텐서보드와 wandb에 측정값을 기록할 함수도 정의해보자.\n",
    "여기서도 accelerator.is_main_process를 다시 사용해 측정값이 워커마다 저장되지 않고 한 번만 기록되게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45fcff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(step, metrics):\n",
    "    logger.info(f\"Step {step}: {metrics}\")\n",
    "    if accelerator.is_main_process:\n",
    "        wandb.log(metrics)\n",
    "        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1803f4",
   "metadata": {},
   "source": [
    "#### 그다음 사용자 정의 ConstantLanguageDataset 클래스로 훈련 세트와 검증 세트를 위한 데이터로더를 만드는 함수를 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d96a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "def create_dataloaders(dataset_name):\n",
    "    train_data = load_dataset(dataset_name+'-train', split=\"train\", streaming=True)\n",
    "    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=args.seed)\n",
    "    \n",
    "    valid_data = load_dataset(dataset_name+'-valid', split=\"validation\", streaming=True)\n",
    "    \n",
    "    train_dataset = ContantLengthDataset(tokenizer, train_data, seq_length=args.seq_length)\n",
    "    valid_dataset = ContantLengthDataset(tokenizer, valid_data, seq_length=args.seq_length)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
    "    eval_dataloader = DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n",
    "    \n",
    "    return train_dataloader, eval_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8363c25",
   "metadata": {},
   "source": [
    "마지막에 배치 처리를 위해 DataLoader로 감싼다.  \n",
    "액셀러레이트가 배치를 각 워커로 분산해줄 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38d71e",
   "metadata": {},
   "source": [
    "또 하나 구현할 것은 최적화다.\n",
    "#### 메인 루프에서 옵티마이저와 학습률을 설정하겠지만, 가중치 감쇠(weight decay)를 받아야 하는 파라미터를 구분하기 위해 헬퍼 함수를 정의하겠다.  \n",
    "보통 절편과 LayerNorm의 가중치에는 가중치 감쇠를 적용하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7c3c88da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters(): # 이름(name)과 파라미터(param)를 반환\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    \n",
    "    return [{'params': params_with_wd, 'weight_decay': args.weight_decay},\n",
    "            {'params': params_without_wd, 'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79d6ac",
   "metadata": {},
   "source": [
    "#### 이따금 검증 세트에서 모델을 평가해야 한다.\n",
    "따라서 평가 세트에서 손실과 복잡도(perplexity)를 계산하는 평가 함수를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5166608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss.repeat(args.valid_batch_size)\n",
    "        losses.append(accelerator.gather(loss))\n",
    "        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b91b1",
   "metadata": {},
   "source": [
    "복잡도는 모델의 출력 확률 분포가 얼마나 타깃 토큰을 잘 예측하는지 측정한다.  \n",
    "복잡도가 낮을수록 성능이 더 좋다.  \n",
    "복잡도는 모델 출력에서 얻은 크로스 엔트로피 손실에 지수 함수를 적용해 계산한다.  \n",
    "특히 훈련 초기에 손실이 높을 때 복잡도를 계산하면 수치적으로 오버플로가 발생할 수 있다.  \n",
    "이 오류를 캐치해서 이 경우 복잡도를 무한대로 설정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c73a9",
   "metadata": {},
   "source": [
    "훈련 스크립트에 이를 모두 적용하기 전에 추가로 사용할 기능이 하나 있다.  \n",
    "허깅페이스 허브는 모델과 데이터셋을 저장하고 버전을 관리하기 위해 내부적으로 깃을 사용한다.  \n",
    "huggingface_hub 라이브러리의 Repository를 사용해 프로그래밍적으로 저장소에 접근하고 풀, 브랜치, 커밋, 푸시를 할 수 있다.  \n",
    "  \n",
    "#### 스크립트에 이를 이용해 훈련하는 동안 모델 체크포인트를 지속적으로 허브에 푸시할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3074cf",
   "metadata": {},
   "source": [
    "### 이제 헬퍼 함수가 모두 준비됐으니 훈련 스크립트의 핵심 부분을 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4206cc11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m samples_per_step \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mnum_processes \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mtrain_batch_size\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 로깅\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m logger, tb_writer, run_name \u001b[38;5;241m=\u001b[39m setup_logging(\u001b[43mproject_name\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      9\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(accelerator\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 모델과 토크나이저를 로드한다.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'project_name' is not defined"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "# 엑셀러레이트\n",
    "accelerator = Accelerator()\n",
    "samples_per_step = accelerator.state.num_processes * args.train_batch_size\n",
    "\n",
    "# 로깅\n",
    "logger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# 모델과 토크나이저를 로드한다.\n",
    "if accelerator.is_main_process:\n",
    "    hf_repo = Repository(\"./\", clone_from=project_name, revision=run_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./\", gradient_checkpointing=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./\")\n",
    "\n",
    "# 데이터셋과 데이터로더를 로드한다.\n",
    "train_dataloader, eval_dataloader = create_dataloaders(dataset_name)\n",
    "\n",
    "# 옵티마이저와 학습률 스케줄러를 준비한다.\n",
    "optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\n",
    "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n",
    "                             num_warmup_steps=args.num_warmup_steps,\n",
    "                             num_training_steps=args.max_train_steps)\n",
    "\n",
    "def get_lr():\n",
    "    return optimizer.params_groups[0]['lr']\n",
    "\n",
    "# 'accelerator'로 모든 것을 준비한다.(매개변수 순서는 중요하지 않습니다.)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "# 모델을 훈련한다.\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    loss = model(batch, labels=batch).loss\n",
    "    log_metrics(step, {'lr':get_lr(),\n",
    "                       'samples': step*samples_per_step,\n",
    "                       'steps': completed_steps,\n",
    "                       'loss/train': loss.item()})\n",
    "    loss = loss / args.gradient_accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "    \n",
    "    if step % args.save_checkpoint_steps == 0:\n",
    "        logger.info(\"Evaluating and saving model checkpoint\")\n",
    "        eval_loss, perplexity = evaluate()\n",
    "        log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model.save_pretrained(\"./\")\n",
    "            hf_repo.push_to_hub(commit_message=f\"step {step}\")\n",
    "        model.train()\n",
    "    if completed_steps >= args.max_train_steps:\n",
    "        break\n",
    "\n",
    "# 마지막 체크포인트를 평가하고 저장한다.\n",
    "logger.info(\"Evaluating and saving model after training\")\n",
    "eval_loss, perplexity = evaluate()\n",
    "log_metrics(step, {\"loss/eval\": eval_loss, 'perplexity': perplexity})\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrapped_model(model)\n",
    "if accelerator.is_main_process:\n",
    "    unwrapped_model.save_pretrained(\"./\")\n",
    "    hf_repo.push_to_hub(commit_message=f\"final model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441289ab",
   "metadata": {},
   "source": [
    "### 코드가 꽤 길지만, 분산 인프라에서 대규모 언어 모델을 훈련할 때 필요한 코드 전부다. 이 스크립트를 조금씩 나눠서 가장 중요한 부분을 설명하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef76279",
   "metadata": {},
   "source": [
    "### 모델저장\n",
    "-> 모델 저장소 내에서 이 스크립트를 실행하면 스크립트가 시작되면서 wandb에서 얻은 run_name을 따라 새 브랜치를 체크아웃한다.  \n",
    "-> 나중에 체크포인트마다 모델을 커밋하고 허브에 푸시한다.  \n",
    "-> 이런 방식으로 실험할 때마다 새 브랜치가 만들어지고 각 커밋은 모델 체크포인트에 해당한다.  \n",
    "-> 모델을 저장할 때 제대로 동기화되도록 wait_fro_everyone()과 unwrap_model()을 호출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a0d64",
   "metadata": {},
   "source": [
    "### 최적화\n",
    "-> 모델 최적화를 위해 선형적인 워밍업 단계를 거친 후, 코사인 학습률 스케쥴러와 함께 AdamW를 사용한다.  \n",
    "-> 하이퍼파라미터의 경우 GPT-3 논문에 기술된 비슷한 크기의 모델에서 사용한 파라미터와 비슷하게 설정함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f9f38",
   "metadata": {},
   "source": [
    "### 평가\n",
    "-> 모델을 저장할 때마다 평가 세트에서 평가한다.  \n",
    "-> 즉 각 save_checkpoint_steps마다, 그리고 훈련이 끝난 후에 평가한다.  \n",
    "-> 검증 손실과 함께 검증 복잡도도 기록한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af929793",
   "metadata": {},
   "source": [
    "### 그레디언트 누적과 체크포인팅\n",
    "-> 최신 GPU에서 실행하더라도 GPU 메모리가 필요한 배치 크기에 맞지 않는다.  \n",
    "-> 따라서 몇번의 역방향 패스에서 그레디언트를 모아 그레디언트가 충분히 누적됐을 때 최적화 단계를 수행하는 그레디언트 누적을 구현한다.  \n",
    "-> Trainer를 사용해 이를 수행하는 방법은 6장에서 보았다.  \n",
    "  \n",
    "-> 대규모 모델의 경우 단일 배치조차도 하나의 GPU에 맞지 않다.  \n",
    "-> 이 경우 '그레디언트 체크포인팅 gradient checkpointing'이란 방법을 사용해 훈련 속도를 약 20% 낮추면서 메모리 사용량을 줄일 수 있다. (https://oreil.ly/94oj1)  \n",
    "-> 이를 사용하면 더 큰 모델도 단일 GPU에서 훈련할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7ae52",
   "metadata": {},
   "source": [
    "#### 다중 GPU에서 모델을 훈련하는 방법 -> 그중에서도 액셀러레이트에서 사용하는 방법\n",
    "DataDistributedParallelism(DDP)  \n",
    "https://oreil.ly/m4iNm  \n",
    "단일 GPU에 맞는 것보다 더 큰 배치에서 모델을 빠르게 훈련한다는 장점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9f98d",
   "metadata": {},
   "source": [
    "#### 모델이 하나의 GPU에 들어갈 수 없을 때는 좀 더 정교한 병렬화 전략이 필요하다.\n",
    "https://oreil.ly/3uhfq -> 허깅페이스에서 문서가 삭제된거 같은데 parallelism 파트를 더 찾아봐야할듯?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e1acb7",
   "metadata": {},
   "source": [
    "## 10.3.5 훈련 실행\n",
    "훈련 서버에서 실행되도록 훈련 스크립트를 codeparrot_training.py 파일에 저장하겠다.  \n",
    "이 스크립트와 함게 필요한 파이썬 패키지가 모두 들어있는 파일을 허브의 모델 저장소에 추가하겠다.  \n",
    "허브에 있는 모델은 근본적으로 깃 저장소이므로 이 저장소를 클론하고 원하는 어떤 파일을 추가해 허브에 다시 푸시할 수 있다.  \n",
    "  \n",
    "훈련 서버에서 다음과 같은 몇 개의 명령으로 훈련을 시작한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07000277",
   "metadata": {},
   "source": [
    "\\$ git clone https://huggingface.co/transformersbook/codeparrot  \n",
    "  \n",
    "\\$ cd codeparrot  \n",
    "  \n",
    "\\$ pip install -r requirements.txt  \n",
    "\n",
    "\\$ wandb login  \n",
    "  \n",
    "\\$ accelerate config  \n",
    "  \n",
    "\\$ accelerate launch codeparrot_training.py  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298bd3c",
   "metadata": {},
   "source": [
    "이게 전부다.  \n",
    "wandb login 명령을 실행하면 로깅을 위해 wandb 계정을 인증해야 한다.  \n",
    "accelerate config 명령은 인프라 설정 과정을 안내해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a39b21",
   "metadata": {},
   "source": [
    "#### 전체 훈련 실행이 성공적으로 완료된 후 다음 명령으로 이 실험 브랜치를 메인 브랜치로 머지해 허브로 푸시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c519b0",
   "metadata": {},
   "source": [
    "$ git checkout main  \n",
    "  \n",
    "$ git merge <RUN_NAME>\n",
    "  \n",
    "$ git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3702af0",
   "metadata": {},
   "source": [
    "RUN_NAME은 머지하려는 허브의 실험 브랜치 이름이어야 한다.  \n",
    "이제 훈련된 모델이 있으므로 성능을 조사하는 방법을 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32f3d1",
   "metadata": {},
   "source": [
    "# 10.4 결과 및 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e4639",
   "metadata": {},
   "source": [
    "정성적인 분석과 정량적인 분석을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a34e4a",
   "metadata": {},
   "source": [
    "#### 정성적인 분석\n",
    "구체적인 샘플을 사용해서 모델의 성공 사례와 실패 사례를 더 잘 이해하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc995d41",
   "metadata": {},
   "source": [
    "#### 정량적인 분석\n",
    "대규모 테스트 세트에서 통계적으로 모델의 성능을 평가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d618f",
   "metadata": {},
   "source": [
    "#### 먼저 샘플 몇 개를 살펴보고 모델을 체계적이고 안정적으로 평가하는 방법을 논의한다.  \n",
    "파이프라인으로 작은 모델을 감싸고 이어서 샘플 코드 입력을 전달해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2cbb0065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b632541fa247483c856d3cad2ed95fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/865 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456972bd73304da4a4e7bede2ff4cbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/457M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ddd9766c6243fca5f51691d53015eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0a3af79aec4bfd969f6b0206323d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/497k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a2c72427854528a93365191d2d89e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/277k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca75f217af834d0e954bfd50e6cc0a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/840k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef6da070a7440c487e474291eaaf563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "model_ckpt = 'transformersbook/codeparrot-small'\n",
    "generation = pipeline(\"text-generation\", model=model_ckpt, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25c59b",
   "metadata": {},
   "source": [
    "주어진 프롬프트에서 완성된 코드 후보를 생성하기 위해 생성 파이프라인을 사용한다.  \n",
    "기본적으로 이 파이프라인은 사전에 정의된 길이에 도달할 떄까지 코드를 생성한다.  \n",
    "출력은 여러 개의 함수나 클래스를 포함할 수 있다.  \n",
    "따라서 출력을 간략하게 유지하기 위해 정규식으로 첫째로 등장한 함수나 클래스를 추출하는 first_block() 함수를 구현하겠다.  \n",
    "그 아래 complete_code() 함수는 이 로직을 적용하고 CodeParrot이 생성한 코드 완성을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "db8e67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import set_seed\n",
    "\n",
    "def first_block(string):\n",
    "    return re.split('\\nclass|\\ndef|\\n#|\\n@|\\nprint|\\nif', string)[0].rstrip()\n",
    "\n",
    "def complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):\n",
    "    set_seed(seed)\n",
    "    gen_kwargs = {\"temperature\": 0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1, \"do_sample\":True}\n",
    "    code_gens = generation(prompt, num_return_sequences=num_completions, max_length=max_length, **gen_kwargs)\n",
    "    code_strings = []\n",
    "    for code_gen in code_gens:\n",
    "        generated_code = first_block(code_gen[\"generated_text\"][len(prompt):])\n",
    "        code_strings.append(generated_code)\n",
    "    print(('\\n' + '='*80 + '\\n').join(code_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feccb240",
   "metadata": {},
   "source": [
    "#### 간단한 샘플로 모델이 사각형의 면적을 계산하는 함수를 작성하도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df8d04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    return math.sqrt(a * b)\n",
      "================================================================================\n",
      "\n",
      "    return a * b / 2.0\n",
      "================================================================================\n",
      "\n",
      "    return a * b / 2.0\n",
      "================================================================================\n",
      "\n",
      "    return a / b\n"
     ]
    }
   ],
   "source": [
    "prompt = '''def area_of_rectangle(a: float, b: float):\n",
    "    \"\"\"Return the area of the rectangle.\"\"\"'''\n",
    "complete_code(generation, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f78da7",
   "metadata": {},
   "source": [
    "생성된 코드가 모두 맞지는 않지만 이중에 정답이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52682120",
   "metadata": {},
   "source": [
    "#### 그럼 모델이 HTML에서 URL을 추출하는 조금 더 어려운 작업도 시켜보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a1cb216c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    if not html:\n",
      "        return []\n",
      "    return [url for url in re.findall(r'<a href=\"(/[^/]+/[^\"]+?)\">', html)]\n",
      "================================================================================\n",
      "\n",
      "    return [url for url in re.findall(r'<a href=\"(.*?)\" class=\"url\">(.*?)</a>', html)]\n",
      "================================================================================\n",
      "\n",
      "    if not html:\n",
      "        return []\n",
      "    return [url for url in re.findall(r'<a href=\"([^\"']+)\">', html)]\n",
      "================================================================================\n",
      "\n",
      "    return [url for url in re.findall(r'<a href=\"(.*?)\"', html)]\n"
     ]
    }
   ],
   "source": [
    "prompt = '''def get_urls_from_html(html):\n",
    "    \"\"\"Get all embedded URLs in a HTML string.\"\"\"'''\n",
    "complete_code(generation, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b4417",
   "metadata": {},
   "source": [
    "#### 허깅페이스 홈페이지로 이 함수를 테스트해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9546c342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/models|/spaces/HuggingFaceH4/open_llm_leaderboard|/spaces/facebook/seamless_m4t|/spaces/multimodalart/LoraTheExplorer|/spaces/HuggingFaceM4/idefics_playground|/spaces/fffiloni/Image-to-Story|/spaces|/datasets|/docs/transformers|/docs/diffusers|/docs/safetensors|/docs/huggingface_hub|/docs/tokenizers|/docs/peft|/docs/transformers.js|/docs/timm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_urls_from_html(html):\n",
    "    return [url for url in re.findall(r'<a href=\"(.*?)\"', html)]\n",
    "\n",
    "print(\"|\".join(get_urls_from_html(requests.get('https://hf.co/').text)))\n",
    "# https://로 시작하는 URL은 외부페이지, 나머지는 웹사이트 내 하위 페이지여야 하는데 외부페이지 부분이 안나타나네 흠..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca57084",
   "metadata": {},
   "source": [
    "#### 마지막으로 순수한 파이썬 함수를 넘파이를 사용하는 함수로 바꿀 수 있는지 알아보자\n",
    "다음 코드 블록에서 대용량 GPT-2 체크포인트를 메모리에 로드합니다. 코랩이나 캐글 같은 플랫폼에서는 램이나 GPU 메모리가 부족하기 때문에 인스턴스가 종료될 수 있습니다. 이런 경우 model_ckpt = \"transformersbook/codeparrot-small\"로 바꾸어 작은 체크포인트를 사용하면 이 예제를 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f963a6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadba1ad95794581abfe45d48b5381b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/959 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11f185ddebf4eb88260b1ea011e8313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26ade84746a41e3bcd913b3a2dd726b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec57fc388834325a72139d52aea535e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/497k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5b8cdd9f324b588a8b90fad548cdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/277k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc1e4eb9e6e4d889a3341bd39531ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/840k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273cab5150b0431fb8c109900a7f738d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    return np.mean(a)\n",
      "================================================================================\n",
      "\n",
      "    return sum(a)/len(a)\n",
      "================================================================================\n",
      "\n",
      "    return np.mean(a)\n",
      "================================================================================\n",
      "\n",
      "    return a/len(a)\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"transformersbook/codeparrot\"\n",
    "generation = pipeline('text-generation', model=model_ckpt, device=0)\n",
    "\n",
    "prompt = '''# a function in native python:\n",
    "def mean(a):\n",
    "    return sum(a)/len(a)\n",
    "\n",
    "# the same function using numpy:\n",
    "import numpy as np\n",
    "def mean(a):'''\n",
    "\n",
    "complete_code(generation, prompt, max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209163f",
   "metadata": {},
   "source": [
    "#### 이번에는 CodeParrot 모델이 사이킷런 모델도 만들 수 있는지 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bb0e0357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "reg = DummyRegressor()\n",
      "\n",
      "forest = RandomForestClassifier(n_estimators=20)\n",
      "\n",
      "forest.fit(X, y)\n",
      "================================================================================\n",
      "\n",
      "clf = ExtraTreesClassifier(n_estimators=100, max_features='sqrt')\n",
      "clf.fit(X, y)\n",
      "================================================================================\n",
      "\n",
      "clf = RandomForestClassifier(n_estimators=20)\n",
      "clf.fit(X, y)\n",
      "================================================================================\n",
      "\n",
      "regr_1 = DecisionTreeClassifier(max_depth=4)\n",
      "\n",
      "regr_2 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=4),\n",
      "                          n_estimators=300, random_state=0)\n",
      "\n",
      "regr_1.fit\n"
     ]
    }
   ],
   "source": [
    "prompt = '''X = np.random.randn(100, 100)\n",
    "y = np.random.randint(0, 1, 100)\n",
    "\n",
    "# fit random forest classifier with 20 estimators'''\n",
    "complete_code(generation, prompt, max_length=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b9dc1",
   "metadata": {},
   "source": [
    "두 번째 경우는 ExtraTreesClassifier를 훈련하려고 시도했으며,  \n",
    "네 번째 경우는 2개의 DecisionTree 모델(하나는 DecisionTree, 하나는 AdaBoost)을 서로 섞으려고 한듯하다.  \n",
    "다른 경우는 요청한 대로 코드를 생성했다.(두 번째와 네 번째도 랜덤포레스트와 비슷한 역할을 하긴 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339cce1",
   "metadata": {},
   "source": [
    "#### 생성된 텍스트의 품질을 측정하는 방법은 5장에서 알아봤다.  \n",
    "#### 그중 하나로 많이 사용되는 BLEU 지표는 일반적인 한계가 있기도 하지만, 이 문제에는 특히 잘 맞지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50475829",
   "metadata": {},
   "source": [
    "BLEU 점수는 참조 텍스트와 생성된 텍스트 간의 n-gram 중복을 측정한다.  \n",
    "코드를 작성할 때 변수와 클래스에 자우도가 많고 프로그램에 일관성이 있다면, 이름을 짓는 방식으로 성공 여부가 결정되지 않는다.  \n",
    "하지만 BLEU는 참조 코드에 있는 이름과 다른 이름으로 생성한 결과에 불리하다.  \n",
    "(사실 이런 이름은 거의 예측하기가 불가능하다. (실제 프로그래머도 예측 못함))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff1e8a",
   "metadata": {},
   "source": [
    "#### 소프트웨어 개발 분야에는 유닛 테스트(unit test)와 같이 코드 품질을 측정할 때 신뢰할 만한 좋은 방법이 있다.  \n",
    "이런 방법으로 OpenAI Codex 모델을 평가했다.  \n",
    "코딩 작업을 위해 생성한 여러 코드를 일련의 단위 테스트를 통해 실행하고 테스트를 통과한 비율을 계산한다.  \n",
    "이 장에서 만든 모델의 성능을 제대로 측정하려면 동일한 평가 방식을 적용해야 하겠지만, 이 내용은 이 장의 범위를 넘어선다.  \n",
    "자세한 내용은 CodeParrot이 HumanEval 벤치마크에서 어떻게 동작하는지를 다룬 블로그 포스트  \n",
    "https://oreil.ly/hKOP8 을 참고할 것.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878fd374",
   "metadata": {},
   "source": [
    "# 10.5 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabcd22",
   "metadata": {},
   "source": [
    "이 장에서 수행한 일을 정리해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa313a2d",
   "metadata": {},
   "source": [
    "여기서는 파이썬 코드 자동완성 함수를 만들어보았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5833e4",
   "metadata": {},
   "source": [
    "먼저 대규모 언어 모델을 사전 훈련하기 위해 대규모 데이터셋을 직접 구축했다.  \n",
    "  \n",
    "그다음 이 데이터셋으로 파이썬 코드를 효율적으로 인코딩하는 사용자 정의 토크나이저를 만들었다.  \n",
    "  \n",
    "마지막으로 액셀러레이트를 사용해 모든 것을 연결하고 훈련 스크립트를 작성해서 200줄 미만의 코드로 다중 GPU 인프라에서 소규모와 대규모 GPT-2 모델을 밑바닥부터 훈련했다.  \n",
    "  \n",
    "모델 출력을 조사하면서 모델이 적절하게 이어지는 코드를 생성할 수 있음을 보았고 모델을 체계적으로 평가하는 방법을 논의했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a437bf3",
   "metadata": {},
   "source": [
    "이제 허브에 있는 많은 pretrained model을 fine-tuning 하는 법만이 아니라, 충분한 데이터와 컴퓨팅 자원이 있을 때 자신만의 모델을 밑바닥부터 사전 훈련하는 방법도 배웠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb0024",
   "metadata": {},
   "source": [
    "-> 트랜스포머 모델로 거의 모든 NLP 문제를 다룰 준비를 마친 셈이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
